{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3853186c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example 06 - Wideband Modulations Signal Detector\n",
    "This notebook walks through the process of using TorchSig to instantiate the WBSig53 dataset, load a pretrained DETR model, train the DETR model for signal detection, and evaluate its performance through plots and mean average precision (mAP) scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c1adfb-b2f7-42d2-bd83-c445093a9bed",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Import Libraries\n",
    "First, import all the necessary public libraries as well as a few classes from the `torchsig` toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60290c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvanhoy/.virtual_envs/python38/lib/python3.8/site-packages/torchsig/datasets/wideband_sig53.py:9: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "/home/gvanhoy/.virtual_envs/python38/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/gvanhoy/.virtual_envs/python38/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'criterion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchsig\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msignal_processing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msp\u001b[39;00m \u001b[39mimport\u001b[39;00m Normalize\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchsig\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m Compose\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchsig\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspectrogram_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdetr\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m SetCriterion\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchsig\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspectrogram_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdetr\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdetr\u001b[39;00m \u001b[39mimport\u001b[39;00m detr_b0_nano, format_preds, format_targets\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n",
      "File \u001b[0;32m~/.virtual_envs/python38/lib/python3.8/site-packages/torchsig/models/spectrogram_models/detr/modules.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m box_cxcywh_to_xyxy, generalized_box_iou\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m is_dist_avail_and_initialized, get_world_size, accuracy\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcriterion\u001b[39;00m \u001b[39mimport\u001b[39;00m nested_tensor_from_tensor_list, dice_loss\n\u001b[1;32m     16\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mConvDownSampler\u001b[39;00m(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     17\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, in_chans, embed_dim, ds_rate\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'criterion'"
     ]
    }
   ],
   "source": [
    "from torchsig.datasets.wideband_sig53 import WidebandSig53\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsig.transforms.target_transforms.target_transforms import DescToBBoxSignalDict\n",
    "from torchsig.transforms.expert_feature.eft import Spectrogram\n",
    "from torchsig.transforms.signal_processing.sp import Normalize\n",
    "from torchsig.transforms.transforms import Compose\n",
    "from torchsig.models.spectrogram_models.detr.modules import SetCriterion\n",
    "from torchsig.models.spectrogram_models.detr.detr import detr_b0_nano, format_preds, format_targets\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab25c8-180c-4e59-8055-d9265bd66667",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Instantiate WBSig53 Dataset\n",
    "Here, we instantiate the WBSig53 dataset for training and validation. Please see example notebook 03 for more details on WBSig53. If you plan to compare your results with the baseline performance metrics, please use the impaired datasets by setting `impaired = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cef0f1-2d6c-4090-aedb-8fbc9f443ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify WidebandSig53 Options\n",
    "root = 'wideband_sig53/'\n",
    "train = True\n",
    "impaired = False\n",
    "fft_size = 512\n",
    "num_classes = 1\n",
    "\n",
    "transform = Compose([\n",
    "    Spectrogram(nperseg=fft_size, noverlap=0, nfft=fft_size, mode='complex'),\n",
    "    Normalize(norm=np.inf, flatten=True),\n",
    "])\n",
    "\n",
    "target_transform = Compose([\n",
    "    DescToBBoxSignalDict(),\n",
    "])\n",
    "\n",
    "# Instantiate the training WidebandSig53 Dataset\n",
    "wideband_sig53_train = WidebandSig53(\n",
    "    root=root, \n",
    "    train=train, \n",
    "    impaired=impaired,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    "    regenerate=False,\n",
    "    use_signal_data=True,\n",
    "    gen_batch_size=1,\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "# Instantiate the validation WidebandSig53 Dataset\n",
    "train = False\n",
    "wideband_sig53_val = WidebandSig53(\n",
    "    root=root, \n",
    "    train=train, \n",
    "    impaired=impaired,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    "    regenerate=False,\n",
    "    use_signal_data=True,\n",
    "    gen_batch_size=1,\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve a sample and print out information\n",
    "idx = np.random.randint(len(wideband_sig53_val))\n",
    "data, label = wideband_sig53_val[idx]\n",
    "print(\"Training Dataset length: {}\".format(len(wideband_sig53_train)))\n",
    "print(\"Validation Dataset length: {}\".format(len(wideband_sig53_val)))\n",
    "print(\"Data shape: {}\".format(data.shape))\n",
    "print(\"Label: {}\".format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656424f-f14d-46de-bd28-471772c8e27a",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Format Dataset for Training\n",
    "Next, the datasets are then wrapped as `DataLoaders` to prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf06854-b22f-4269-8661-e9cab52b39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=wideband_sig53_train,\n",
    "    batch_size=4,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=wideband_sig53_val,\n",
    "    batch_size=4,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ba10b-b06b-4b3d-86f7-48e04e6a98d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Instantiate Supported TorchSig Model\n",
    "Below, we load a pretrained DETR-B0-Nano model, and then conform it to a PyTorch LightningModule for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd5975-8df8-415b-b77c-4e9b730bf9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = detr_b0_nano(\n",
    "    pretrained=True,\n",
    "    path=\"detr_b0_nano.pt\",\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62868ef-ceba-4a0c-a2ea-0197493c6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDETR(pl.LightningModule):\n",
    "    def __init__(self, model, data_loader, val_data_loader):\n",
    "        super(ExampleDETR, self).__init__()\n",
    "        self.mdl = model\n",
    "        self.data_loader = data_loader\n",
    "        self.val_data_loader = val_data_loader\n",
    "        self.loss_fn = SetCriterion()\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = data_loader.batch_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mdl(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(x)\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.data_loader\n",
    "    \n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        x = torch.stack([torch.as_tensor(xi, device=\"cuda\") for xi in x], dim=0)\n",
    "        y_hat = self.forward(x)\n",
    "        loss_vals = self.loss_fn(y_hat, y)\n",
    "        loss = self.loss_fn.weight_dict[\"loss_ce\"] * loss_vals[\"loss_ce\"] + \\\n",
    "            self.loss_fn.weight_dict[\"loss_bbox\"] * loss_vals[\"loss_bbox\"] + \\\n",
    "            self.loss_fn.weight_dict[\"loss_giou\"] * loss_vals[\"loss_giou\"]\n",
    "        return {'loss':loss}\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.val_data_loader\n",
    "    \n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        x = torch.stack([torch.as_tensor(xi, device=\"cuda\") for xi in x], dim=0)\n",
    "        y_hat = self.forward(x)\n",
    "        loss_vals = self.loss_fn(y_hat, y)\n",
    "        loss = self.loss_fn.weight_dict[\"loss_ce\"] * loss_vals[\"loss_ce\"] + \\\n",
    "            self.loss_fn.weight_dict[\"loss_bbox\"] * loss_vals[\"loss_bbox\"] + \\\n",
    "            self.loss_fn.weight_dict[\"loss_giou\"] * loss_vals[\"loss_giou\"]\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return {'val_loss': loss}\n",
    "        \n",
    "example_model = ExampleDETR(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5575fc-7629-4a24-900a-e405b512bff4",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Train the Model\n",
    "To train the model, we first create a `ModelCheckpoint` to monitor the validation loss over time and save the best model as we go. The network is then instantiated and passed into a `Trainer` to kick off training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d926e9-bc15-4f4a-a27e-c0b8e24845c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup checkpoint callbacks\n",
    "checkpoint_filename = \"{}/checkpoints/checkpoint\".format(os.getcwd())\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filename=checkpoint_filename,\n",
    "    save_top_k=True,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "# Create and fit trainer\n",
    "epochs = 50\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    callbacks=checkpoint_callback,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[2],\n",
    "    accumulate_grad_batches=4,\n",
    ")\n",
    "trainer.fit(example_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8edf8-dc0a-41bc-bf86-1ee2dc76f0ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Evaluate the Trained Model\n",
    "Once the network is fully trained, we load the best checkpoint and then infer over a few random samples, inspecting the performance through plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93867565-0ade-4687-b2b2-23de73a6c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "checkpoint = torch.load(checkpoint_filename+\".ckpt\", map_location=lambda storage, loc: storage)\n",
    "example_model.load_state_dict(checkpoint['state_dict'])\n",
    "example_model = example_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd5c210-cffa-46b2-a67d-24f4b253db2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds = {}\n",
    "data_collection = {}\n",
    "label_collection = {}\n",
    "results_collection = {}\n",
    "\n",
    "threshold = 0.8\n",
    "fft_size = 512\n",
    "\n",
    "num_eval = 6\n",
    "for p in range(num_eval):\n",
    "    # Retrieve data\n",
    "    idx = np.random.randint(len(wideband_sig53_val))\n",
    "    data, label = wideband_sig53_val[idx]\n",
    "    data_collection[p] = data\n",
    "    \n",
    "    # Infer\n",
    "    with torch.no_grad():\n",
    "        data = torch.from_numpy(np.expand_dims(data,0)).float()\n",
    "        data = data.cuda() if torch.cuda.is_available() else data\n",
    "        pred = example_model(data)\n",
    "    preds[p] = pred\n",
    "    \n",
    "    # Convert output to detections dataframe\n",
    "    component_num = 0\n",
    "    column_names = [\"DetectionIdx\", \"Probability\", \"CenterTimePixel\", \"DurationPixel\", \"CenterFreqPixel\", \"BandwidthPixel\", \"Class\"]\n",
    "    detected_signals_df = pd.DataFrame(columns = column_names)\n",
    "    \n",
    "    # Loop over the number of objects DETR outputs\n",
    "    for obj_idx in range(pred['pred_logits'].shape[1]):\n",
    "        probs = pred['pred_logits'][0][obj_idx].softmax(-1)\n",
    "        max_prob = probs.max().cpu().detach().numpy()\n",
    "        max_class = probs.argmax().cpu().detach().numpy()\n",
    "        \n",
    "        # If max class is not the last class for no object, interpret values\n",
    "        if max_class != (pred['pred_logits'].shape[2] - 1) and max_prob > threshold:\n",
    "            center_time = pred['pred_boxes'][0][obj_idx][0]\n",
    "            center_freq = pred['pred_boxes'][0][obj_idx][1]\n",
    "            duration = pred['pred_boxes'][0][obj_idx][2]\n",
    "            bandwidth = pred['pred_boxes'][0][obj_idx][3]\n",
    "        \n",
    "            # Save to dataframe\n",
    "            detected_signals_df.at[component_num,\"DetectionIdx\"] = component_num\n",
    "            detected_signals_df.at[component_num,\"Probability\"] = max_prob\n",
    "            detected_signals_df.at[component_num,\"CenterTimePixel\"] = center_time.cpu().detach().numpy() * fft_size\n",
    "            detected_signals_df.at[component_num,\"DurationPixel\"] = duration.cpu().detach().numpy() * fft_size\n",
    "            detected_signals_df.at[component_num,\"CenterFreqPixel\"] = center_freq.cpu().detach().numpy() * fft_size\n",
    "            detected_signals_df.at[component_num,\"BandwidthPixel\"] = bandwidth.cpu().detach().numpy() * fft_size\n",
    "            detected_signals_df.at[component_num,\"Class\"] = max_class\n",
    "            component_num += 1\n",
    "\n",
    "    # Save to results collection\n",
    "    results_collection[p] = detected_signals_df\n",
    "    \n",
    "    # Convert label to labels dataframe\n",
    "    component_num = 0\n",
    "    labels_df = pd.DataFrame(columns = column_names)\n",
    "    \n",
    "    for label_obj_idx in range(len(label['labels'])):\n",
    "        center_time = label[\"boxes\"][label_obj_idx][0]\n",
    "        center_freq = label[\"boxes\"][label_obj_idx][1]\n",
    "        duration = label[\"boxes\"][label_obj_idx][2]\n",
    "        bandwidth = label[\"boxes\"][label_obj_idx][3]\n",
    "        class_name = label[\"labels\"][label_obj_idx]\n",
    "\n",
    "        # Save to dataframe\n",
    "        labels_df.at[component_num,\"DetectionIdx\"] = component_num\n",
    "        labels_df.at[component_num,\"Probability\"] = 1.0\n",
    "        labels_df.at[component_num,\"CenterTimePixel\"] = center_time * fft_size\n",
    "        labels_df.at[component_num,\"DurationPixel\"] = duration * fft_size\n",
    "        labels_df.at[component_num,\"CenterFreqPixel\"] = center_freq * fft_size\n",
    "        labels_df.at[component_num,\"BandwidthPixel\"] = bandwidth * fft_size\n",
    "        labels_df.at[component_num,\"Class\"] = class_name\n",
    "        component_num += 1\n",
    "\n",
    "    # Save to label collection\n",
    "    label_collection[p] = labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1311f5d-d3e3-4630-9fe1-2590617e9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_annotation = False\n",
    "\n",
    "plt.figure(figsize=(15, 25))\n",
    "for i in range(num_eval):\n",
    "    ax = plt.subplot(num_eval,3,i+1)\n",
    "    \n",
    "    # Convert complex spectrogram to magnitude for plotting\n",
    "    data_plot = np.squeeze(data_collection[i])\n",
    "    data_plot = data_plot[0]**2 + data_plot[1]**2\n",
    "    data_plot = 20*np.log10(data_plot)\n",
    "\n",
    "    # Retrieve individual label\n",
    "    ax.imshow(data_plot)\n",
    "    for sig_idx in range(results_collection[i].shape[0]):\n",
    "        rect = mpl.patches.Rectangle(\n",
    "            (results_collection[i].iloc[sig_idx][\"CenterTimePixel\"]-results_collection[i].iloc[sig_idx][\"DurationPixel\"]/2,\n",
    "             results_collection[i].iloc[sig_idx][\"CenterFreqPixel\"]-results_collection[i].iloc[sig_idx][\"BandwidthPixel\"]/2),\n",
    "            results_collection[i].iloc[sig_idx][\"DurationPixel\"],\n",
    "            results_collection[i].iloc[sig_idx][\"BandwidthPixel\"],\n",
    "            linewidth=1,\n",
    "            edgecolor='r',\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        if include_annotation:\n",
    "            ax.annotate(\n",
    "                \"{:.1f}%\".format(results_collection[i].iloc[sig_idx][\"Probability\"]*100), \n",
    "                (\n",
    "                    results_collection[i].iloc[sig_idx][\"CenterTimePixel\"]+results_collection[i].iloc[sig_idx][\"DurationPixel\"]/2, \n",
    "                    results_collection[i].iloc[sig_idx][\"CenterFreqPixel\"]-results_collection[i].iloc[sig_idx][\"BandwidthPixel\"]/2\n",
    "                ), \n",
    "                color='w', \n",
    "                weight='bold', \n",
    "                fontsize=8, \n",
    "                ha='right', \n",
    "                va='bottom',\n",
    "             )\n",
    "        \n",
    "    plt.title(\"DETR Bounding Boxes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd21b745-1b3d-46bb-8885-33019413c1bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Compute the Mean Average Precision\n",
    "As a final evaluation technique, we use the TorchMetrics's `MeanAveragePrecision` metric for computing the mAP. Please note that the TorchMetrics mAP computation is fairly slow, but it is the recommended tool for comparing to the performance baselines we provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220061d-3d61-4102-9452-5289f941bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mAP_metric = MeanAveragePrecision(class_metrics=False)\n",
    "\n",
    "fft_size = 512\n",
    "batch_size = 32\n",
    "num_eval = len(wideband_sig53_val)\n",
    "data_idx = 0\n",
    "\n",
    "fp16 = True\n",
    "\n",
    "model = model.eval().cuda()\n",
    "if fp16:\n",
    "    # Note: only the backbone supports fp16 precision at this time\n",
    "    model.backbone = model.backbone.half()\n",
    "    model.conv = model.conv.half()\n",
    "    model.transformer = model.transformer.float()\n",
    "    model.linear_class = model.linear_class.float()\n",
    "    model.linear_bbox = model.linear_bbox.float()\n",
    "else:\n",
    "    model.backbone = model.backbone.float()\n",
    "    model.conv = model.conv.float()\n",
    "    model.transformer = model.transformer.float()\n",
    "    model.linear_class = model.linear_class.float()\n",
    "    model.linear_bbox = model.linear_bbox.float()\n",
    "\n",
    "for curr_batch in tqdm(range(num_eval // batch_size)):\n",
    "    # Create batch\n",
    "    batch = np.zeros((batch_size, 2, fft_size, fft_size))\n",
    "    label_batch = []\n",
    "    for batch_element in range(batch_size):\n",
    "        # Retrieve data\n",
    "        idx = data_idx if num_eval == len(wideband_sig53_val) else np.random.randint(len(wideband_sig53_val))\n",
    "        data_idx += 1\n",
    "        data, label = wideband_sig53_val[idx]\n",
    "        batch[batch_element,:] = data\n",
    "        label_batch.append(label)\n",
    "    \n",
    "    # Infer\n",
    "    with torch.no_grad():\n",
    "        model_input = torch.from_numpy(batch)\n",
    "        model_input = model_input.cuda() if torch.cuda.is_available() else model_input\n",
    "        if fp16:\n",
    "            x = model.backbone(model_input.half())\n",
    "            h = model.conv(x)\n",
    "            h = model.transformer(h.float()).float()\n",
    "            preds = {\n",
    "                'pred_logits': model.linear_class(h), \n",
    "                'pred_boxes': model.linear_bbox(h).sigmoid()\n",
    "            }\n",
    "        else:\n",
    "            preds = model(model_input.float())\n",
    "        \n",
    "    # Format the predictions to match the torchmetrics input format\n",
    "    map_preds = format_preds(preds)\n",
    "    map_targets = format_targets(label_batch)\n",
    "    mAP_score = mAP_metric.update(map_preds, map_targets)\n",
    "    \n",
    "# Calc mAP\n",
    "print(\"Computing metrics...\")\n",
    "start_time = time.time()\n",
    "mAP_dict = mAP_metric.compute()\n",
    "mAP_score = float(mAP_dict['map'].numpy())\n",
    "print(\"Done computing metrics in {:.2f}s\".format(time.time() - start_time))\n",
    "\n",
    "print(\"mAP: {}\".format(mAP_score))\n",
    "mAP_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d853db-b0a2-4c01-8c78-a3f70f2cf4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
