{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b0d326",
   "metadata": {},
   "source": [
    "# Importing External Data into TorchSig: Bring Your Own Data (BYOD) NumPy\n",
    "This notebook shows how to import externally created data into TorchSig using a basic NumPy data plus JSON metadata example file format.\n",
    "\n",
    "---\n",
    "\n",
    "The main code that the user must write is a subclass of `ExternalFileHandler`, which will be passed into a `ExternalTorchSigDataset`. The `ExternalFileHandler` class must implement 3 methods:\n",
    "| Method | Arguments | Return | Description |\n",
    "| ------ | --------- | ------ | ----------- |\n",
    "| `size` | N/A | int | Number of data samples, dataset size |\n",
    "| `load_dataset_metadata` | N/A | `ExternalDatasetMetadata` | Dataset information, see `datasets/dataset_metadata.py` for more information. |\n",
    "| `load` | idx: int | (np.ndarray, List[Any]) | Load sample `idx`, which includes data as np.ndarray and taregts as a list. |\n",
    "\n",
    "If you want to apply TorchSig's transforms and impairments to your data, note that `load` must return targets that are in `List[Dict]` format, where each dict describes a signal. Additionally, the dict must have the fields required by each transform, e.g., `FamilyName` target transform requires the signal to have `class_name` in its metadata. It is up to the user to figure out what metadata is needed for what transforms/target transforms they wish to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06acb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from typing import Tuple, Dict, List, Any\n",
    "import itertools\n",
    "import pprint\n",
    "\n",
    "# TorchSig\n",
    "from torchsig.datasets.datasets import ExternalTorchSigDataset\n",
    "from torchsig.datasets.dataset_metadata import ExternalDatasetMetadata\n",
    "from torchsig.utils.file_handlers import ExternalFileHandler\n",
    "from torchsig.transforms.transforms import ComplexTo2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11e075-256b-4847-add2-f1fbb512f2f0",
   "metadata": {},
   "source": [
    "## Step 1: External Data Generation Process: create synthetic data outside TorchSig workflow\n",
    "\n",
    "If your data already exists somewhere, you can skip to Step 2.\n",
    "\n",
    "We will write a sample dataset using Numpy's npy for signal data and and csv for metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6981f-7ac9-4635-8849-d5df6d7281f5",
   "metadata": {},
   "source": [
    "### External Synthetic Data and Metadata Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters\n",
    "root = 'datasets/byod_npy_example'   # data file top-level folder \n",
    "seed = 1234567890                    # rng seed\n",
    "\n",
    "os.makedirs(root, exist_ok=True)     # directory for files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fabf89",
   "metadata": {},
   "source": [
    "Below, we generate some signals (outside of TorchSig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be33642-0514-4eae-8552-b7ddc8eb4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "fs = 1_000_000                              # 1 MHz sample-rate (fixed rate)\n",
    "num_samples = 1024                          # samples per data (fixed size)\n",
    "dataset_size = 8                            # dataset size\n",
    "labels = ['BPSK', 'QPSK', 'Noise']          # three arbitrary metadata class labels (strings)\n",
    "modcod = [0, 1, 2]                          # three arbitrary metadata integers\n",
    "rng = np.random.default_rng(seed)           # random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4801f20-ff14-4265-94b6-78cef460d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user's external data: non-TorchSig synthetic data along with metadata\n",
    "\n",
    "signals_array = np.empty((dataset_size, num_samples), dtype=np.complex64)  # store all data in memory\n",
    "meta_rows = []                                           # store all metadata in memory\n",
    "\n",
    "t = np.arange(num_samples) / fs  # timesteps\n",
    "\n",
    "# create dataset\n",
    "for idx in range(dataset_size):\n",
    "    label = rng.choice(labels)\n",
    "    mc = rng.choice(modcod)\n",
    "    \n",
    "    if label == \"BPSK\":\n",
    "        bits   = rng.integers(0, 2, num_samples)\n",
    "        sig    = (2*bits-1) + 0j\n",
    "    elif label == \"QPSK\":\n",
    "        bits   = rng.integers(0, 4, num_samples)\n",
    "        table  = {0:1+1j, 1:1-1j, 2:-1+1j, 3:-1-1j}\n",
    "        sig    = np.vectorize(table.get)(bits)\n",
    "    else:  # white noise\n",
    "        sig = (rng.normal(size=num_samples) + 1j*rng.normal(size=num_samples)) * 0.1\n",
    "    \n",
    "    sig /= np.sqrt((np.abs(sig)**2).mean()) # normalize power for consistency\n",
    "    signals_array[idx] = sig.astype(np.complex64)\n",
    "    \n",
    "    # add to metadata\n",
    "    meta_rows.append(\n",
    "        dict(\n",
    "            index=idx, \n",
    "            label=label, \n",
    "            modcod=mc, \n",
    "            sample_rate=fs\n",
    "        )\n",
    "    )\n",
    "\n",
    "# write information about dataset\n",
    "global_metadata = {\n",
    "    \"size\": dataset_size,\n",
    "    \"num_samples\": num_samples,\n",
    "    \"class_labels\": labels,\n",
    "    \"sample_rate\": fs\n",
    "}\n",
    "with open(f\"{root}/info.json\", 'w') as f:\n",
    "    json.dump(global_metadata, f, indent=4)\n",
    "\n",
    "# write data as npy\n",
    "np.save(f\"{root}/data.npy\", signals_array)\n",
    "\n",
    "# write metadata\n",
    "with open(f\"{root}/metadata.csv\", 'w', newline='') as f:\n",
    "    csv.DictWriter(f, fieldnames=meta_rows[0].keys()).writerows(meta_rows)\n",
    "\n",
    "print(f\"Synthetic signals + metadata staged in {root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8869659d-2933-4d72-8de8-318aeae9db3b",
   "metadata": {},
   "source": [
    "## Step 2. ExternalFileHandler\n",
    "\n",
    "To have your data on disk interface with TorchSig, you must write your own `ExternalFileHandler` so TorchSig knows how to handle your data. Make sure to call `super()`.\n",
    "\n",
    "Note that the metadata must at least have:\n",
    "- `class_name`\n",
    "- `class_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77031156",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYODExampleFileHandler(ExternalFileHandler):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str\n",
    "    ):\n",
    "        super().__init__(root=root)\n",
    "\n",
    "        self.class_list = ['BPSK', 'QPSK', 'Noise']  \n",
    "\n",
    "    def size(self) -> int:\n",
    "        try:\n",
    "            with open(f\"{self.root}/info.json\", \"r\") as f:\n",
    "                dataset_info = json.load(f)\n",
    "\n",
    "            return dataset_info[\"size\"]\n",
    "        except:\n",
    "            raise ValueError(f\"Error loading {root}/info.json\")\n",
    "    \n",
    "    def load_dataset_metadata(self) -> ExternalDatasetMetadata:\n",
    "        try:\n",
    "            with open(f\"{self.root}/info.json\", \"r\") as f:\n",
    "                dataset_info = json.load(f)\n",
    "\n",
    "            return ExternalDatasetMetadata(\n",
    "                # minimum fields required for ExternalDatasetMetadata\n",
    "                num_iq_samples_dataset = dataset_info[\"num_samples\"],\n",
    "                sample_rate = dataset_info[\"sample_rate\"],\n",
    "                class_list = dataset_info[\"class_labels\"],\n",
    "                num_samples = dataset_info[\"size\"]\n",
    "            )\n",
    "        except:\n",
    "            raise ValueError(f\"Error loading {self.root}/info.json\")\n",
    "\n",
    "    def load(self, idx: int) -> Tuple[np.ndarray, List[Dict]]:\n",
    "        try:\n",
    "            # loads entire data to access an element: inefficient, but acceptable for a\n",
    "            # small basic example - use memory mapping or another format for better efficiency\n",
    "            data = np.load(f\"{self.root}/data.npy\")[idx]\n",
    "\n",
    "            with open(f\"{self.root}/metadata.csv\", \"r\") as f:\n",
    "                reader = csv.DictReader(f, fieldnames=[\"index\", \"label\", \"modcod\", \"sample_rate\"])\n",
    "                # get to idx row\n",
    "                row = next(itertools.islice(reader, idx, idx+1), None)\n",
    "                if row is None:\n",
    "                    raise IndexError(f\"Metadata idx {idx} is out of bounds\")\n",
    "\n",
    "                row[\"index\"] = int(row[\"index\"])\n",
    "                row[\"sample_rate\"] = float(row[\"sample_rate\"])\n",
    "                # add class_name\n",
    "                row[\"class_name\"] = row[\"label\"].lower()\n",
    "                # add class index\n",
    "                row[\"class_index\"] = self.class_list.index(row[\"label\"])\n",
    "\n",
    "                metadata = row\n",
    "\n",
    "            return data, [metadata]\n",
    "        except:\n",
    "            raise ValueError(f\"Error loading {root}/info.json\")\n",
    "\n",
    "test = BYODExampleFileHandler(root)\n",
    "print(f'Size: {test.size()}')\n",
    "print(f'Metadata: {test.load_dataset_metadata()}')\n",
    "print(f'Load element 2: {test.load(2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0e530-9864-475b-8ef1-eee3df5751e6",
   "metadata": {},
   "source": [
    "## Step 3: ExternalTorchSigDataset\n",
    "\n",
    "Use `ExternalTorchSigDataset` and custom file handler (above) to load in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b885c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'datasets/byod_npy_example'    \n",
    "\n",
    "custom_dataset = ExternalTorchSigDataset(\n",
    "    file_handler = BYODExampleFileHandler(root),\n",
    "    target_labels = None\n",
    ")\n",
    "print(f\"Dataset size: {len(custom_dataset)}\")\n",
    "\n",
    "sample = custom_dataset[4]\n",
    "print(f\"data: {sample.data}\")\n",
    "print(f\"metadata: {[meta.to_dict() for meta in sample.get_full_metadata()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can apply transforms and metadata transforms\n",
    "root = 'datasets/byod_npy_example'    \n",
    "\n",
    "custom_dataset_2 = ExternalTorchSigDataset(\n",
    "    file_handler = BYODExampleFileHandler(root),\n",
    "    transforms = [ComplexTo2D()],\n",
    "    target_labels = [\"modcod\"]\n",
    ")\n",
    "print(f\"Dataset size: {len(custom_dataset_2)}\")\n",
    "\n",
    "data, metadata = custom_dataset_2[4]\n",
    "print(f\"data: {data.shape}\")\n",
    "print(f\"metadata: {metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905bda01-1ced-4485-b5ab-1381ce2dc8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d2518-4ae1-4fa2-922d-54129cd4bc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
