{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b0d326",
   "metadata": {},
   "source": [
    "# Importing External Data into TorchSig: Bring Your Own Data (BYOD) NumPy Example\n",
    "This notebook shows an example of how to import externally created data into TorchSig using a basic NumPy dataset with JSON metadata file format. \n",
    "\n",
    "This example employs a provided NumPy `NPYReader` subclass of TorchSig's `FileReader` to read a custom externally created dataset as a `StaticTorchSigDataset`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06acb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, csv, json, math\n",
    "\n",
    "# TorchSig\n",
    "from torchsig.utils.file_handlers.npy import NPYReader\n",
    "from torchsig.datasets.datasets import StaticTorchSigDataset\n",
    "from torchsig.transforms.transforms import ComplexTo2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11e075-256b-4847-add2-f1fbb512f2f0",
   "metadata": {},
   "source": [
    "## Step 1: External Data Generation Process: create synthetic data outside TorchSig workflow\n",
    "\n",
    "If your data already exists somewhere, you can skip to Step 2.\n",
    "\n",
    "We will write a sample dataset using Numpy's npy for signal data and and csv for metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6981f-7ac9-4635-8849-d5df6d7281f5",
   "metadata": {},
   "source": [
    "### External Synthetic Data and Metadata Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration parameters\n",
    "root = \"datasets/byod_npy_example\"  # data file top-level folder\n",
    "seed = 1234567890  # rng seed\n",
    "\n",
    "os.makedirs(root, exist_ok=True)  # directory for files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fabf89",
   "metadata": {},
   "source": [
    "Below, we generate some signals (outside of TorchSig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be33642-0514-4eae-8552-b7ddc8eb4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "fs = 1_000_000  # 1 MHz sample-rate (fixed rate)\n",
    "num_iq_samples = 1024  # I/Q samples per data element (fixed size)\n",
    "dataset_size = 8  # number of total data elements in dataset\n",
    "elements_per_file = 2  # number of data elements per .npy file\n",
    "num_files = math.ceil(dataset_size / elements_per_file)\n",
    "\n",
    "labels = [\"BPSK\", \"QPSK\", \"Noise\"]  # three arbitrary metadata class labels (strings)\n",
    "modcod = [0, 1, 2]  # three arbitrary metadata integers\n",
    "rng = np.random.default_rng(seed)  # random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4801f20-ff14-4265-94b6-78cef460d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some external data: non-TorchSig synthetic data along with limited metadata\n",
    "\n",
    "signals_array = np.empty(\n",
    "    (dataset_size, num_iq_samples), dtype=np.complex64\n",
    ")  # during generation, store all data in memory\n",
    "meta_rows = []  # during generation, store all metadata in memory\n",
    "\n",
    "t = np.arange(num_iq_samples) / fs  # timesteps\n",
    "\n",
    "# create dataset\n",
    "for idx in range(dataset_size):\n",
    "    label = rng.choice(labels)\n",
    "    mc = rng.choice(modcod)\n",
    "\n",
    "    if label == \"BPSK\":\n",
    "        bits = rng.integers(0, 2, num_iq_samples)\n",
    "        sig = (2 * bits - 1) + 0j\n",
    "    elif label == \"QPSK\":\n",
    "        bits = rng.integers(0, 4, num_iq_samples)\n",
    "        table = {0: 1 + 1j, 1: 1 - 1j, 2: -1 + 1j, 3: -1 - 1j}\n",
    "        sig = np.vectorize(table.get)(bits)\n",
    "    else:  # white noise\n",
    "        sig = (\n",
    "            rng.normal(size=num_iq_samples) + 1j * rng.normal(size=num_iq_samples)\n",
    "        ) * 0.1\n",
    "\n",
    "    sig /= np.sqrt((np.abs(sig) ** 2).mean())  # normalize power for consistency\n",
    "    signals_array[idx] = sig.astype(np.complex64)\n",
    "\n",
    "    # add to metadata\n",
    "    meta_rows.append(dict(index=idx, label=label, modcod=mc, sample_rate=fs))\n",
    "\n",
    "# write dataset-level metadata to JSON\n",
    "global_metadata = {\n",
    "    \"size\": dataset_size,\n",
    "    \"num_iq_samples\": num_iq_samples,\n",
    "    \"num_files\": num_files,\n",
    "    \"elements_per_file\": elements_per_file,\n",
    "    \"class_labels\": labels,\n",
    "    \"sample_rate\": fs,\n",
    "}\n",
    "with open(os.path.join(root, \"info.json\"), \"w\") as f:\n",
    "    json.dump(global_metadata, f, indent=4)\n",
    "\n",
    "# write data as multiple .npy files\n",
    "for i in range(num_files):\n",
    "    start = i * elements_per_file\n",
    "    end = min(dataset_size, (i + 1) * elements_per_file)\n",
    "    chunk = signals_array[start:end]  # slice the chunk for current file\n",
    "    filename = os.path.join(root, f\"data_{i}.npy\")\n",
    "    np.save(filename, chunk)  # save chunk to .npy\n",
    "\n",
    "\n",
    "# write sample-specific metadata to CSV\n",
    "with open(os.path.join(root, \"metadata.csv\"), \"w\", newline=\"\") as csvfile:\n",
    "    csv.DictWriter(csvfile, fieldnames=meta_rows[0].keys()).writerows(meta_rows)\n",
    "\n",
    "print(f\"Synthetic signals + metadata staged in {root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8869659d-2933-4d72-8de8-318aeae9db3b",
   "metadata": {},
   "source": [
    "## Step 2. FileReader\n",
    "To enable your data on disk to interface with TorchSig, you may use one of the provided `FileReader` examples or write your own `FileReader` so TorchSig knows how to handle your data. Make sure to call `super()` in your own implementation. For this example we will use the provided `NPYReader` reader class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0e530-9864-475b-8ef1-eee3df5751e6",
   "metadata": {},
   "source": [
    "## Step 3: StaticTorchSigDataset\n",
    "\n",
    "Use `StaticTorchSigDataset` and a file handler to interface with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b885c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"datasets/byod_npy_example\"\n",
    "\n",
    "custom_dataset = StaticTorchSigDataset(\n",
    "    file_handler_class=NPYReader, root=root, target_labels=None\n",
    ")\n",
    "print(f\"Dataset size: {len(custom_dataset)}\")\n",
    "\n",
    "sample = custom_dataset[4]\n",
    "print(f\"Data: {sample.data}\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can apply transforms and metadata transforms\n",
    "root = \"datasets/byod_npy_example\"\n",
    "\n",
    "custom_dataset_2 = StaticTorchSigDataset(\n",
    "    file_handler_class=NPYReader,\n",
    "    root=root,\n",
    "    transforms=[ComplexTo2D()],\n",
    "    target_labels=[\"modcod\"],\n",
    ")  # transform complex data to 2D format  # return custom label\n",
    "print(f\"Dataset size: {len(custom_dataset_2)}\")\n",
    "\n",
    "data, label = custom_dataset_2[4]\n",
    "print(f\"Data element shape: {data.shape}\")\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d49d1-7701-4956-a051-9e98d4ef6c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "kernelspec": {
  "display_name": "Python 3",
  "language": "python",
  "name": "python3"
 },
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}