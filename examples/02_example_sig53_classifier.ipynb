{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3853186c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example 02 - Sig53 Classifier\n",
    "This notebook walks through a simple example of how to use the clean Sig53 dataset, load a pre-trained supported model, and evaluate the trained network's performance. Note that the experiment and the results herein are not to be interpreted with any significant value but rather serve simply as a practical example of how the `torchsig` dataset and tools can be used and integrated within a typical [PyTorch](https://pytorch.org/) and/or [PyTorch Lightning](https://www.pytorchlightning.ai/) workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c1adfb-b2f7-42d2-bd83-c445093a9bed",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Import Libraries\n",
    "First, import all the necessary public libraries as well as a few classes from the `torchsig` toolkit. An additional import from the `cm_plotter.py` helper script is also done here to retrieve a function to streamline plotting of confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60290c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsig.models.iq_models.efficientnet.efficientnet import efficientnet_b4\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchsig.utils.cm_plotter import plot_confusion_matrix\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from sklearn.metrics import classification_report\n",
    "from torchsig.datasets.sig53 import Sig53\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchsig.transforms as ST\n",
    "import numpy as np\n",
    "import torchsig\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab25c8-180c-4e59-8055-d9265bd66667",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Instantiate Sig53 Dataset\n",
    "Here, we instantiate the Sig53 clean training dataset and the Sig53 clean validation dataset. We demonstrate how to compose multiple TorchSig transforms together, using a data impairment with a random phase shift that uniformly samples a phase offset between -1 pi and +1 pi. The next transform normalizes the complex tensor, and the final transform converts the complex data to a real-valued tensor with the real and imaginary parts as two channels. We additionally provide a target transform that maps the `SignalDescription` objects, that are part of `SignalData` objects, to a desired format for the model we will train. In this case, we use the `DescToClassIndex` target transform to map class names to their indices within an ordered class list. Finally, we sample from our datasets and print details in order to confirm functionality.\n",
    "\n",
    "For more details on the Sig53 dataset instantiations, please see the Sig53 example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cef0f1-2d6c-4090-aedb-8fbc9f443ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Sig53 Options\n",
    "root = \"sig53/\"\n",
    "train = False\n",
    "impaired = False\n",
    "class_list = list(Sig53._idx_to_name_dict.values())\n",
    "transform = ST.Compose([\n",
    "    ST.RandomPhaseShift(phase_offset=(-1, 1)),\n",
    "    ST.Normalize(norm=np.inf),\n",
    "    ST.ComplexTo2D(),\n",
    "])\n",
    "target_transform = ST.DescToClassIndex(class_list=class_list)\n",
    "\n",
    "# Instantiate the Sig53 Clean Training Dataset\n",
    "sig53_clean_train = Sig53(\n",
    "    root=root, \n",
    "    train=train, \n",
    "    impaired=impaired,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    "    use_signal_data=True,\n",
    ")\n",
    "\n",
    "# Instantiate the Sig53 Clean Validation Dataset\n",
    "train = False\n",
    "sig53_clean_val = Sig53(\n",
    "    root=root, \n",
    "    train=train, \n",
    "    impaired=impaired,\n",
    "    transform=transform,\n",
    "    target_transform=target_transform,\n",
    "    use_signal_data=True,\n",
    ")\n",
    "\n",
    "# Retrieve a sample and print out information to verify\n",
    "idx = np.random.randint(len(sig53_clean_train))\n",
    "data, label = sig53_clean_train[idx]\n",
    "print(\"Dataset length: {}\".format(len(sig53_clean_train)))\n",
    "print(\"Data shape: {}\".format(data.shape))\n",
    "print(\"Label Index: {}\".format(label))\n",
    "print(\"Label Class: {}\".format(Sig53.convert_idx_to_name(label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656424f-f14d-46de-bd28-471772c8e27a",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Format Dataset for Training\n",
    "Next, the datasets are then wrapped as `DataLoaders` to prepare for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf06854-b22f-4269-8661-e9cab52b39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=sig53_clean_train,\n",
    "    batch_size=16,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=sig53_clean_val,\n",
    "    batch_size=16,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9f859-d71b-4c78-95a0-3587c1e4db30",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Instantiate Supported TorchSig Model\n",
    "Below, we load a pretrained EfficientNet-B4 model, and then conform it to a PyTorch LightningModule for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d071b0-a5a1-4290-bb32-48745906b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efficientnet_b4(\n",
    "    pretrained=False,\n",
    "    path=\"efficientnet_b4.pt\",\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a77adc-6604-40e2-b0f6-da3385472fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleNetwork(LightningModule):\n",
    "    def __init__(self, model, data_loader, val_data_loader):\n",
    "        super(ExampleNetwork, self).__init__()\n",
    "        self.mdl = model\n",
    "        self.data_loader = data_loader\n",
    "        self.val_data_loader = val_data_loader\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.lr = 0.001\n",
    "        self.batch_size = data_loader.batch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mdl(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            out = self.forward(x)\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.data_loader\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y = torch.squeeze(y.to(torch.int64))\n",
    "        loss = F.cross_entropy(self(x.float()), y)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_data_loader\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        y = torch.squeeze(y.to(torch.int64))\n",
    "        val_loss = F.cross_entropy(self(x.float()), y)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        return {\"val_loss\": val_loss}\n",
    "\n",
    "example_model = ExampleNetwork(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5575fc-7629-4a24-900a-e405b512bff4",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Train the Model\n",
    "To train the model, we first create a `ModelCheckpoint` to monitor the validation loss over time and save the best model as we go. The network is then instantiated and passed into a `Trainer` to kick off training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d926e9-bc15-4f4a-a27e-c0b8e24845c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup checkpoint callbacks\n",
    "checkpoint_filename = \"{}/checkpoints/checkpoint\".format(os.getcwd())\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filename=checkpoint_filename,\n",
    "    save_top_k=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "# Create and fit trainer\n",
    "epochs = 25\n",
    "trainer = Trainer(\n",
    "    max_epochs=epochs, callbacks=checkpoint_callback, accelerator=\"gpu\", devices=1\n",
    ")\n",
    "trainer.fit(example_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8edf8-dc0a-41bc-bf86-1ee2dc76f0ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "### Evaluate the Trained Model\n",
    "After the model is trained, the checkpoint's weights are loaded into the model and the model is put into evaluation mode. The validation set is looped through, inferring results for each example and saving the predictions and the labels. Finally, the labels and predictions are passed into our confusion matrix plotting function to view the results and also passed into the `sklearn.metrics.classification_report` method to print metrics of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93867565-0ade-4687-b2b2-23de73a6c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "checkpoint = torch.load(checkpoint_filename+\".ckpt\", map_location=lambda storage, loc: storage)\n",
    "example_model.load_state_dict(new_state_dict, strict=False)\n",
    "example_model = example_model.eval()\n",
    "example_model = example_model.cuda() if torch.cuda.is_available() else example_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd5c210-cffa-46b2-a67d-24f4b253db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer results over validation set\n",
    "num_test_examples = len(sig53_clean_val)\n",
    "num_classes = len(list(Sig53._idx_to_name_dict.values()))\n",
    "y_raw_preds = np.empty((num_test_examples,num_classes))\n",
    "y_preds = np.zeros((num_test_examples,))\n",
    "y_true = np.zeros((num_test_examples,))\n",
    "\n",
    "for i in tqdm(range(0,num_test_examples)):\n",
    "    # Retrieve data\n",
    "    idx = i # Use index if evaluating over full dataset\n",
    "    data, label = sig53_clean_val[idx]\n",
    "    # Infer\n",
    "    data = torch.from_numpy(np.expand_dims(data,0)).float()\n",
    "    data = data.cuda() if torch.cuda.is_available() else data\n",
    "    pred_tmp = example_model.predict(data)\n",
    "    pred_tmp = pred_tmp.cpu().numpy() if torch.cuda.is_available() else pred_tmp\n",
    "    # Argmax\n",
    "    y_preds[i] = np.argmax(pred_tmp)\n",
    "    # Store label\n",
    "    y_true[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba708833-a460-4eb8-b87b-ec7f3ae8377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.sum(np.asarray(y_preds)==np.asarray(y_true))/len(y_true)\n",
    "plot_confusion_matrix(\n",
    "    y_true, \n",
    "    y_preds, \n",
    "    classes=class_list,\n",
    "    normalize=True,\n",
    "    title=\"Example Modulations Confusion Matrix\\nTotal Accuracy: {:.2f}%\".format(acc*100),\n",
    "    text=False,\n",
    "    rotate_x_text=90,\n",
    "    figsize=(16,9),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154e551-2760-4195-9950-22ca6be3d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1311f5d-d3e3-4630-9fe1-2590617e9534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c91ae-2eec-4e36-a930-dabd5e57c045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
