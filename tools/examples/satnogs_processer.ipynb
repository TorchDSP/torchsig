{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eceef8d",
   "metadata": {},
   "source": [
    "# SatNOGS Processor\n",
    "\n",
    "This notebook showcases how to use SatNOGS (Satellite Networked Open Ground Station) with TorchSig. This downloads 1 observations from satnogs and creates a spectrogram and PNGs file. Then allows the user to draw bounding boxes on the spectrogram, the spectrogram is processed into Torchsig to make predictions. The torchsig predictions are given back to the user for further refinement via sigmf.\n",
    "\n",
    "Note:  This notebook must be ran in Jupyter notebook due to Yolo annotation tool and may not work properly in VS code \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc2e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fbc94-6c9f-4874-a8e9-ea33757dd123",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This notebook grabs signals from Satnogs and then converts it to sigmf and then runs it through Torchsig\"\"\"\n",
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import librosa\n",
    "import cv2\n",
    "from scipy import signal\n",
    "import torchaudio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyter_bbox_widget import BBoxWidget\n",
    "from sigmf import SigMFFile, sigmffile\n",
    "from torchsig.image_datasets.datasets.yolo_datasets import YOLOFileDataset\n",
    "from torchsig.image_datasets.annotation_tools.yolo_annotation_tool import yolo_annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05162f-c885-4f65-a750-58036220fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date\n",
    "now = datetime.today()\n",
    "yesterday = now - timedelta(days=1)\n",
    "date =  yesterday.strftime(\"%Y-%m-%d\")\n",
    "PAGENUM = 1\n",
    "NEXTPAGE = 1\n",
    "waterfall_links = []\n",
    "BASE_SATNOGS_URL = 'https://network.satnogs.org/observations/'\n",
    "x = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8701c-3b7c-41b1-9fa5-3ea4fe3f393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while NEXTPAGE == 1 and x == 1:\n",
    "    URELDATE = f'https://network.satnogs.org/observations/?norad=&start={date}'\n",
    "    URLRES = '+18%3A55&end=&observer=&station=&results=w1&results=a1&results=d1&rated=rw1&'\n",
    "    MAINURL = URELDATE + URLRES + f'transmitter_mode=&transmitter_uuid=&page={PAGENUM}'\n",
    "\n",
    "    print(f\"\\nCurrently on page: {PAGENUM}\")\n",
    "    print(f\"Current item count: {x}\")\n",
    "\n",
    "    try:\n",
    "        mainresponse = requests.get(MAINURL, timeout=10)\n",
    "\n",
    "        if mainresponse.status_code == 200:\n",
    "            # Parse the HTML content of the main page\n",
    "            mainsoup = BeautifulSoup(mainresponse.text, 'html.parser')\n",
    "            disable_next = mainsoup.select_one('#page-selector > ul > li.page-item.disabled')\n",
    "            if disable_next and PAGENUM > 1:\n",
    "                NEXTPAGE = 0\n",
    "                print(\"On the last page, setting NEXTPAGE to 0.\")\n",
    "\n",
    "            # Find all elements with the class \"badge badge-good\" (these contain observation IDs)\n",
    "            GoodStatus = mainsoup.find_all(class_='badge badge-good')\n",
    "            for item in GoodStatus:\n",
    "                if x != 1:\n",
    "                    print(f\"Reached item limit of {x}. Stopping inner loop.\")\n",
    "                    break\n",
    "                observation_id = item.text.strip()\n",
    "                ITEMURL = f'https://network.satnogs.org/observations/{observation_id}'\n",
    "\n",
    "                print(f\"  Processing observation ID: {observation_id}\")\n",
    "                try:\n",
    "                    itemresponse = requests.get(ITEMURL, timeout=10)\n",
    "                    if itemresponse.status_code == 200:\n",
    "                        itemsoup = BeautifulSoup(itemresponse.text, 'html.parser')\n",
    "                        title = itemsoup.title\n",
    "                        if title:\n",
    "                            print(f\"    Page Title: {title.text.strip()}\")\n",
    "                        else:\n",
    "                            print(f\"    Warning: Could not find page title for {observation_id}.\")\n",
    "                        data_element = itemsoup.find('a', href=lambda x_href: x_href and\n",
    "                                                                    'satnogs' in x_href and\n",
    "                                                                    x_href.endswith('.ogg'))\n",
    "                        if data_element:\n",
    "                            waterfall_link = data_element['href']\n",
    "                            waterfall_links.append(waterfall_link)\n",
    "                            x += 1\n",
    "                            print(f\"    Successfully found and saved audio link. Total items: {x}\")\n",
    "                        else:\n",
    "                            print(f\"    Audio link ('.ogg') not found for observation {observation_id}.\")\n",
    "                    else:\n",
    "                        print(f\"    Failed to retrieve data for observation {observation_id}. \"\n",
    "                                f\"Status code: {itemresponse.status_code}\")\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"    An error occurred while requesting {ITEMURL}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    An unexpected error occurred for observation {observation_id}: {e}\")\n",
    "            if x != 1:\n",
    "                print(f\"Overall item limit of {x} reached. Setting NEXTPAGE to 0 to stop main loop.\")\n",
    "                NEXTPAGE = 0 #\n",
    "\n",
    "        else:\n",
    "            # If the main page request failed\n",
    "            print(f\"Failed to retrieve main page. Status code: {mainresponse.status_code}\")\n",
    "            NEXTPAGE = 0 # Stop the loop if main page fails\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Catch network errors for the main page request\n",
    "        print(f\"Network error occurred while fetching main page {MAINURL}: {e}\")\n",
    "        NEXTPAGE = 0 # Stop the loop on network error\n",
    "    # Increment to the next page number for the next iteration of the while loop\n",
    "    PAGENUM += 1\n",
    "print(waterfall_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7de26-5570-4546-8c56-13b026bbcc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torchaudioused(torchchunk):\n",
    "    \"\"\"\n",
    "    Determines the normalized spectrogram\n",
    "    \"\"\"\n",
    "\n",
    "    torchnfft = 1024\n",
    "    specgram = torchaudio.transforms.Spectrogram(\n",
    "        n_fft=torchnfft*2,\n",
    "        win_length=torchnfft*2,\n",
    "        hop_length=torchnfft*2,\n",
    "        window_fn=torch.blackman_window,\n",
    "        normalized=False,\n",
    "        center=False,\n",
    "        onesided=True,\n",
    "        power=True,\n",
    "    )\n",
    "    norm = lambda x: torch.linalg.norm(\n",
    "        x,\n",
    "        ord=float(\"inf\"),\n",
    "            keepdim=True,\n",
    "    )\n",
    "    torchx = specgram(torch.from_numpy(torchchunk))\n",
    "    torchx = torchx[:-1]\n",
    "    torchx = torchx * (1 / norm(torchx.flatten()))\n",
    "    torchx = torchx.flipud()\n",
    "    torchx = 10*np.log10(torchx.numpy()+1e-12)\n",
    "    return torchx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e362e0-12de-428d-a186-25d011dc67fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseband_downsample_real_to_real(input_data,r_cfreq,signal_freq,israte,osrate):\n",
    "    \"\"\"\n",
    "    reduces the data and sample rate due to a constraint\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available() is True:\n",
    "        with torch.cuda.device('cuda:0'):\n",
    "            input_data.to('cuda:0')\n",
    "            inx = torch.linspace(0,len(input_data)-1,len(input_data),\n",
    "                               dtype=torch.complex64,device='cuda:0')\n",
    "            fshift = (r_cfreq-signal_freq)/(israte*1.0)\n",
    "            fv = math.e ** (1j*2* math.pi *fshift*inx)\n",
    "            fv.to('cuda:0')\n",
    "            input_data = input_data * fv\n",
    "            num_samples_in = len(input_data)\n",
    "            # print(len(input_data))\n",
    "            num_samples = int(np.ceil(num_samples_in/(israte/osrate)))\n",
    "            # print (int((num_samples*israte)))\n",
    "            osrate_new = int((num_samples*israte)/num_samples_in)\n",
    "            down_factor = int(israte/osrate_new)\n",
    "            transform = torchaudio.transforms.Resample(int(israte/osrate_new),\n",
    "                                                       1,dtype=torch.float32).to('cuda:0')\n",
    "            #test = transform(input_data.real) + 1j*transform(input_data.imag)\n",
    "            test = transform(input_data.real)\n",
    "            return test,(israte/down_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b6c74-91e9-4451-91ae-142ab7a8eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_PNG(directory,sample_rate,pngname,NFFT,x):\n",
    "    \"\"\"\n",
    "    Saves an image\n",
    "    \"\"\"\n",
    "    if sample_rate > 0:\n",
    "        sampleRate.update({pngname: sample_rate})\n",
    "        x = cv2.resize(x, (NFFT, NFFT), interpolation=cv2.INTER_LINEAR)\n",
    "    #convert to 3 channel grayscale black hot image using opencv\n",
    "    pngimg_new = np.zeros((NFFT, NFFT, 3),dtype=np.float32)\n",
    "    pngimg_new = cv2.normalize(x, pngimg_new, 0, 255, cv2.NORM_MINMAX)\n",
    "    pngimg_new = cv2.cvtColor(pngimg_new.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    pngimg_new = cv2.bitwise_not(pngimg_new)\n",
    "    # Create the full path for the new file\n",
    "    imgtext = \"image.png\"\n",
    "    pngimgname =  pngname + imgtext\n",
    "    pngoutput_path = os.path.join(directory,pngimgname)  # Change extension as needed\n",
    "    print(pngoutput_path)\n",
    "    if cv2.imwrite(pngoutput_path, pngimg_new):\n",
    "        print(f\"Image saved successfully: {pngoutput_path}\")\n",
    "    else:\n",
    "        print(\"Failed to save image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073167f-8a62-4a6c-bb89-cfb7c50f7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets constants nfft, Id (current ogg in audio_links, starts at 1)\n",
    "NFFT = 1024\n",
    "NFFTNFFT = NFFT * NFFT * 2\n",
    "ID = 0\n",
    "#determines if the user wants to refine the data,\n",
    "#if they do, It will require further user input.\n",
    "    #Otherwise will run through the entire file automatically\n",
    "edit = input(\"Would you like to manually refine the data Y/N\").upper()\n",
    "# Defines the directory in which final images will be saved in\n",
    "DIRECTORY = 'signals/dataset'\n",
    "if not os.path.exists(DIRECTORY):\n",
    "    os.makedirs(DIRECTORY)\n",
    "\n",
    "refdir = os.path.join(DIRECTORY, 'refined')\n",
    "if not os.path.exists(refdir):\n",
    "    os.makedirs(refdir)\n",
    "\n",
    "undir = os.path.join(DIRECTORY, 'unrefined')\n",
    "if not os.path.exists(undir):\n",
    "    os.makedirs(undir)\n",
    "\n",
    "#iterates through the entire audio_link file\n",
    "#2\n",
    "sampleRate = {}\n",
    "for link in waterfall_links:\n",
    "    response = requests.get(link)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Create a temporary ogg file in the system's temp directory\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.ogg') as temp_file:\n",
    "            temp_file.write(response.content)\n",
    "            fileName = temp_file.name\n",
    "            #converts the ogg data into a numpy array (data) and\n",
    "                #determines the sample_rate for later use.\n",
    "            try:\n",
    "                data, sample_rate = librosa.load(temp_file.name,sr=None)\n",
    "            except:\n",
    "                break\n",
    "        # #Increments id for each new data and creates chunks from the data\n",
    "        print(data)\n",
    "        ID = ID +1\n",
    "        # print(type(data))\n",
    "        print(data[:NFFT*NFFT])\n",
    "        # chunk = data[:NFFT*NFFT]\n",
    "        # print(chunk)\n",
    "        lenSamples = len(data)\n",
    "        chunk_count_rounded = int(lenSamples/NFFTNFFT)\n",
    "        print(int(lenSamples))\n",
    "        data = data[:(chunk_count_rounded*NFFTNFFT)]\n",
    "        data = data.reshape(chunk_count_rounded,NFFTNFFT)\n",
    "        print(data.shape)\n",
    "        # Goes through the chunks of the data file and converts them into usable pngs for yolo.\n",
    "        for i in range(chunk_count_rounded):\n",
    "            IDNAME = str(ID)+\",\"+str(i)\n",
    "            # print(data[i].shape)\n",
    "            chunk = data[i]\n",
    "            #print(chunk.shape)\n",
    "            #print(i)\n",
    "            chunkx = torchaudioused(chunk)\n",
    "            #3\n",
    "            #If the user has requested to refine the data,\n",
    "                #it will pass them to the refine function.\n",
    "            sampleRate.update({IDNAME: sample_rate})\n",
    "            if edit in ('Y', 'YES'):\n",
    "                Save_PNG(undir,0,IDNAME,NFFT,chunkx)\n",
    "            else:\n",
    "                Save_PNG(refdir,0,IDNAME,NFFT,chunkx)\n",
    "                print(\"no refinement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c08949-12b5-41ff-839d-0e284195b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tosigmf (signame,sigidname, manual,sigedit):\n",
    "    \"\"\"\n",
    "      converts to SIGMF file\n",
    "    \"\"\"\n",
    "    sigauthor = \"jane.doe@domain.org\"\n",
    "    sigdes = \"ogg real file.\"\n",
    "    sigver = \"1.0.0\"\n",
    "    sigpngname = signame.replace(\".txt\", \"\")\n",
    "    sigpng_path = os.path.join(refdir, sigpngname)\n",
    "    # Define basic metadata\n",
    "    if signame !='.ipynb_checkpoints':\n",
    "        while sigedit in ('Y', 'YES'):\n",
    "            sigfield = input(\"What field would you like to edit? author, description, or version\")\n",
    "            match sigfield:\n",
    "                case \"author\":\n",
    "                    sigauthor = input(\"What would you like to set as the author of this file?\")\n",
    "                case \"description\":\n",
    "                    sigdes = input(\"What would you like to set as the description of this file?\")\n",
    "                case \"version\":\n",
    "                    sigver = input(\"What would you like to set as the version of this file?\")\n",
    "            sigedit = input(\"Would you like to edit another field? Y/N\")\n",
    "        sigsample_rate = sampleRate[sigidname]\n",
    "        sigmeta = SigMFFile(\n",
    "            global_info = {\n",
    "                SigMFFile.DATATYPE_KEY: 'cf64_le',\n",
    "                SigMFFile.SAMPLE_RATE_KEY: sigsample_rate,\n",
    "                SigMFFile.AUTHOR_KEY: sigauthor,\n",
    "                SigMFFile.DESCRIPTION_KEY: sigdes,\n",
    "                SigMFFile.VERSION_KEY: sigver,\n",
    "            }\n",
    "        )\n",
    "        if not manual:\n",
    "            for siglabel in labels:\n",
    "                i = 0\n",
    "                sx, sigcx, sigcy, sigw, _ = siglabel\n",
    "                sigcx = sigcx * width\n",
    "                sigsx = sigcx - ((sigw * width) // 2)\n",
    "                sigex = sigcx + ((sigw * width) // 2)\n",
    "                sigcy = (sigcy * 2) * int(sigsample_rate /2)\n",
    "\n",
    "                if len(labels) == 1:\n",
    "                    #crops image to only where the signal has been identified, which will prevent the sigmf file from being created\n",
    "                    refdata, refsample_rate = baseband_downsample_real_to_real(torch.from_numpy(chunk[(int(sigsx)*2048):(int(sigex)*2048)]).to('cuda:0'),\n",
    "                                                                               int(int(sample_rate)/4),int(sigcy),48000,1600)\n",
    "                    #overwrites data in annotation dataset\n",
    "                    refdata = refdata[:(chunk_count_rounded*NFFTNFFT)]\n",
    "                    refdata = refdata.cpu().numpy()\n",
    "                    x = torchaudioused(refdata)\n",
    "                    print(\"x\", x)\n",
    "                    Save_PNG(refdir,refsample_rate,sigidname,NFFT,x)\n",
    "\n",
    "                sigmeta.add_capture(sx, metadata={\n",
    "                    \"core:sample_start\":sigsx,\n",
    "                    \"core:sample_count\":1,\n",
    "                    \"core:frequency\": sigcy,\n",
    "                    \"core:comment\": i,\n",
    "                })\n",
    "                sigmeta.validate()\n",
    "                output_path = os.path.join(refdir, metaname)\n",
    "                sigmeta.tofile(output_path)\n",
    "                i = i + 1\n",
    "\n",
    "        else:\n",
    "            sigsx = labels[0]\n",
    "            sigex = labels[1]\n",
    "            sigcy = labels[2]\n",
    "            try:\n",
    "                #crops image to only where the signal has been identified, which will prevent the sigmf file from being created\n",
    "                refdata, refsample_rate = baseband_downsample_real_to_real(torch.from_numpy(chunk[(int(sigsx)*2048):(int(sigex)*2048)]).to('cuda:0'),\n",
    "                                                                           int(int(sample_rate)/4),int(sigcy),48000,1600)\n",
    "                # #overwrites data in annotation dataset\n",
    "                data = refdata[:(chunk_count_rounded*NFFTNFFT)]\n",
    "                x = torchaudioused(data)\n",
    "\n",
    "                Save_PNG(refdir,refsample_rate,sigidname,NFFT,x)\n",
    "                #6\n",
    "                sigmeta.add_capture(sx, metadata={\n",
    "                    \"core:sample_start\":sigsx,\n",
    "                    \"core:sample_count\":1,\n",
    "                    \"core:frequency\": sigcy,\n",
    "                    \"core:comment\": 1,\n",
    "                })\n",
    "                sigmeta.validate()\n",
    "                output_path = os.path.join(refdir, metaname + \".sigmf-meta\")\n",
    "                # print(output_path)\n",
    "                sigmeta.tofile(output_path)\n",
    "            except:\n",
    "                print(\"no meta saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b244048-8e97-4190-8e88-c98174c3e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data that needs refrainment via manual input\n",
    "if edit == \"Y\":\n",
    "    for file in os.listdir(undir):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".png\"):\n",
    "            name = filename.replace(\".png\",\"\")\n",
    "            idname = name.replace(\"image_name\", \"\")\n",
    "            metaname = name + \"sigmf-meta\"\n",
    "            imagepath = os.path.join(undir,filename)\n",
    "            # print(imagepath)\n",
    "            # Load the image\n",
    "            image = cv2.imread(imagepath)\n",
    "            # Adjust the brightness and contrast\n",
    "            # Adjusts the brightness by adding 10 to each pixel value\n",
    "            BRIGHTNESS = 10\n",
    "            # Adjusts the contrast by scaling the pixel values by 2.3\n",
    "            CONTRAST = 2.3\n",
    "            image2 = cv2.addWeighted(image, CONTRAST, np.zeros(image.shape, image.dtype), 0, BRIGHTNESS)\n",
    "\n",
    "            figure = plt.figure(figsize=(30,30))\n",
    "            ax = plt.subplot()\n",
    "            figure.suptitle(\"with axis 48K real\")\n",
    "            ax.imshow(image)\n",
    "            ax.set_xlabel(\"Duration:\")\n",
    "            ax.set_ylabel(\"Bandwidth:\")\n",
    "            ax.set_yticks(list(np.linspace(1023,0,100)),list(np.linspace(0,int(sample_rate/2),100)))\n",
    "            plt.show()\n",
    "            mansx = input(\"Enter start duration\")\n",
    "            manex = input(\"Enter end duration\")\n",
    "            mancy = input(\"Enter frequency\")\n",
    "            manlabels = [mansx,manex,mancy]\n",
    "            tosigmf (name,idname, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997f17f-b414-41d5-940e-4bf6d3b8b78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data that needs refrainment via drawling\n",
    "from torchsig.image_datasets.annotation_tools.yolo_annotation_tool import yolo_annotator\n",
    "if edit == \"Y\":\n",
    "    UNLAB_DIR = \"signals/dataset/unrefined/\" # directory of images to be annotated\n",
    "else:\n",
    "    UNLAB_DIR = \"signals/dataset/refined/\"\n",
    "NEWYOLODT = \"signals/annotated_dataset/\" # directory to save annotated yolo data\n",
    "yolo_annotator(UNLAB_DIR,NEWYOLODT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c74f8-8134-4516-9d5a-4ecbf87bffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data that needs refrainment via drawling\n",
    "from torchsig.image_datasets.datasets.yolo_datasets import YOLOFileDataset\n",
    "plt.rcParams[\"figure.figsize\"] = (50,50)\n",
    "yds1 = YOLOFileDataset(NEWYOLODT)\n",
    "yds1.root_filepath, os.listdir(yds1.root_filepath + \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967abbb-98b7-4406-a9c9-11c1e082d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"current defaults for all files: Author: jane.doe@domain.org Description: ogg real file. Version Key: 1.0.0\") \n",
    "sigedit = input(\"would you like to edit the default fields for all files? Y/N\").upper()\n",
    "\n",
    "for x in range (len(yds1)):\n",
    "    image= yds1[x].img\n",
    "    height, width = image.shape[:2]\n",
    "    labels = yds1[x].labels\n",
    "    name = os.listdir(yds1.root_filepath + \"labels\")[x]\n",
    "    name = name.replace(\".png\",\"\")\n",
    "    idname = name.replace(\"image\", \"\")\n",
    "    idname = idname.replace(\".txt\", \"\")\n",
    "    metaname = name.replace(\".txt\", \".sigmf-meta\")\n",
    "    meta_path = os.path.join(refdir, metaname)\n",
    "    # print(name + \"-\" + idname + \"-\" + metaname)\n",
    "    #metadata\n",
    "    tosigmf(name,idname, False,sigedit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa343b-03c4-4bb9-8ec8-908c6ad8f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueListOfLists:\n",
    "    \"\"\"\n",
    "    Make sure that the list only contains unique values\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._lists = []\n",
    "\n",
    "    def add(self, new_list):\n",
    "        \"\"\"\n",
    "        Adds unique values to the list\n",
    "        \"\"\"\n",
    "        if new_list not in self._lists:\n",
    "            self._lists.append(new_list)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._lists)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"UniqueListOfLists({self._lists})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d17394",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "destination_path=detect.pt\n",
    "download_url=https://bucket.ltsnet.net/torchsig/models/detect.pt\n",
    "\n",
    "curl -L -o \"$destination_path\" \"$download_url\"\n",
    "\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Download completed successfully.\"\n",
    "else\n",
    "    echo \"Download failed.\"\n",
    "fi\n",
    "\n",
    "destination_path=11s.pt\n",
    "download_url=https://bucket.ltsnet.net/torchsig/models/11s.pt\n",
    "\n",
    "curl -L -o \"$destination_path\" \"$download_url\"\n",
    "\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Download completed successfully.\"\n",
    "else\n",
    "    echo \"Download failed.\"\n",
    "fi\n",
    "\n",
    "destination_path=xcit.ckpt\n",
    "download_url=https://bucket.ltsnet.net/torchsig/models/xcit.ckpt\n",
    "\n",
    "curl -L -o \"$destination_path\" \"$download_url\"\n",
    "\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Download completed successfully.\"\n",
    "else\n",
    "    echo \"Download failed.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3419d-64fe-40cc-9ea3-26e5ae4fe2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing sigmf files to model for predictions\n",
    "model = YOLO('detect.pt')\n",
    "result = model.named_parameters\n",
    "# Load a dataset\n",
    "for file in os.listdir(refdir):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith(\"-meta\"):\n",
    "        output_path = os.path.join(refdir,filename)\n",
    "        # print(output_path)\n",
    "\n",
    "        signal = sigmffile.fromfile(output_path)\n",
    "        samples = 1\n",
    "\n",
    "        #removes any current annotations from file\n",
    "        filetxt = filename.replace(\".sigmf-meta\", \".txt\")\n",
    "        file_path = os.path.join(\"signals/annotated_dataset/labels/\",filetxt)\n",
    "        # print(file_path)\n",
    "\n",
    "        fileNamepng = filename.replace(\".sigmf-meta\", \".png\")\n",
    "        fileNamepng = fileNamepng.replace(\".png.png\", \".png\")\n",
    "        file_png_path = os.path.join(\"signals/dataset/refined/\",fileNamepng)\n",
    "        # print(file_png_path)\n",
    "        labels = UniqueListOfLists()\n",
    "        for x in range(len(signal.get_captures())):\n",
    "            # Get some metadata and all annotations\n",
    "            fc = signal.get_captures()[x]['core:frequency']\n",
    "            sample_rate = signal.get_global_field(SigMFFile.SAMPLE_RATE_KEY)\n",
    "            sample_count = signal.get_captures()[x][ \"core:sample_count\"]\n",
    "            signal_duration = sample_count / sample_rate\n",
    "            fs = sample_rate\n",
    "            file_png_path = os.path.join(\"signals/dataset/refined/\",fileNamepng)\n",
    "            if cv2.haveImageReader(file_png_path):\n",
    "                img_new = cv2.imread(file_png_path)\n",
    "            else:\n",
    "                continue\n",
    "            result = model(img_new , imgsz=NFFT, save=False, augment=False,iou=0.1, max_det=300)\n",
    "            plot_img = result[0].orig_img\n",
    "            height, width = plot_img.shape[:2]\n",
    "            print(height,width)\n",
    "            Z = 0\n",
    "            # print(result[0].boxes.xyxy)\n",
    "            if result[0].boxes.xyxy != []:\n",
    "                for Z, boxes_xyxy in enumerate(result[0].boxes.xyxy):\n",
    "                    y = 0\n",
    "                    box_xyxy = boxes_xyxy.cpu().numpy()\n",
    "                    box_xywh = result[0].boxes.xywh[Z].cpu().numpy()\n",
    "                    center_freq = (float(fs)/2.0)-(float(box_xywh[1]/NFFT)*float(fs))+fc\n",
    "                    top_freq = (float(fs)/2.0)-((box_xyxy[1]/NFFT)*float(fs))+fc\n",
    "                    bottom_freq = (float(fs)/2.0)-((box_xyxy[3]/NFFT)*float(fs))+fc\n",
    "                    bandwidth = top_freq - bottom_freq\n",
    "                    start_sample = int(box_xyxy[0])*int(NFFT)\n",
    "                    end_sample = int(box_xyxy[2])*int(NFFT)\n",
    "                    duration = end_sample - start_sample\n",
    "                    signal.add_annotation(start_sample,duration, metadata = {\n",
    "                        SigMFFile.FLO_KEY: bottom_freq,\n",
    "                        SigMFFile.FHI_KEY: top_freq,\n",
    "                        SigMFFile.COMMENT_KEY:str(y),\n",
    "                    })\n",
    "                    # print(box_xyxy[0])\n",
    "                    cx = (box_xyxy[0] + box_xywh[2]//2)/width\n",
    "                    cy = (box_xyxy[1] + box_xywh[3]//2)/height\n",
    "                    # print(cx)\n",
    "                    new_width = box_xywh[2]/width\n",
    "                    # print(new_width)\n",
    "                    new_height = box_xywh[3]/height\n",
    "                    # print(new_height)\n",
    "                    # print(\"end:\" + str(cx))\n",
    "                    print(y)\n",
    "                    labels.add([cx,cy,new_width,new_height])\n",
    "                    y = y + 1\n",
    "                signal.tofile(output_path)\n",
    "            with open(file_path, \"w\") as file:\n",
    "                y = 0\n",
    "                for line in labels:\n",
    "                    file.write(str(y) + \" \"+ str(line[0]) + \" \"+ str(line[1]) + \" \"+str(line[2])+ \" \"+ str(line[3]) + \"\\n\")\n",
    "                    y = y + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e52ee-7904-4c1d-a177-080fe3ca64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates dataset of all annotations images\n",
    "plt.rcParams[\"figure.figsize\"] = (50,50)\n",
    "YOLODIR = \"signals/annotated_dataset/\" # directory to save annotated yolo data\n",
    "yds1 = YOLOFileDataset(YOLODIR )\n",
    "# type(yds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16c8fc-0ba8-4974-ae27-736a9b83af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    yds1.root_filepath, os.listdir(yds1.root_filepath + \"labels\")\n",
    "    yds1[0]\n",
    "    # print(yds1[1].labels)\n",
    "except:\n",
    "    raise ValueError('No annotated images in directory, please either 1) re-run and annotate images or 2) if the tool is unavailable please run in jupyter lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b6491-7770-4610-8d97-bf69eb588970",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range (len(yds1)):\n",
    "    imagename = os.listdir(yds1.root_filepath + \"images\")[x]\n",
    "    imagename =imagename[:-3] + \"png\"\n",
    "    print(imagename)\n",
    "    image= yds1[x].img\n",
    "    boxclass = []\n",
    "    labels = yds1[x].labels\n",
    "    image = 1 - image.numpy()\n",
    "    if labels != [[]]:\n",
    "        # font\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        # fontScale\n",
    "        FONTSCALE = 1\n",
    "        # Blue color in BGR\n",
    "        color = (0, 255, 0)\n",
    "        image = (np.stack([image[0,:,:]]*3).transpose(1,2,0)*255).astype(np.uint8)\n",
    "        for label in labels:\n",
    "            cid, cx, cy, w, h = label\n",
    "            print(label)\n",
    "            img_h, img_w = image.shape[:2]\n",
    "            x1 = int((cx - w/2)*img_w)\n",
    "            x2 = int((cx + w/2)*img_w)\n",
    "            y1 = int((cy - h/2)*img_h)\n",
    "            y2 = int((cy + h/2)*img_h)\n",
    "            image = cv2.rectangle(image.copy(), (x1, y1), (x2, y2), color=(255,0,0), thickness=1)\n",
    "            # org\n",
    "            org = (x1, y1)\n",
    "            # Using cv2.putText() method\n",
    "            image = cv2.putText(image.copy(), str(cid) , org, font, FONTSCALE, color)\n",
    "            print(cid)\n",
    "\n",
    "        plt.imshow(image)\n",
    "        outimg_path = os.path.join(\"signals/dataset/refined\", imagename)  # Change extension as needed\n",
    "        if cv2.imwrite(outimg_path, image):\n",
    "            print(f\"Image saved successfully: {outimg_path}\")\n",
    "        else:\n",
    "            print(\"Failed to save image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40536107-2590-4652-a80c-36982dbce305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the users input on what labels they would like to change\n",
    "changeimagename = input(\"What image would you like to update EX:1,14\")\n",
    "edit = input(\"What labels would you like to edit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf17a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541826db-e898-4936-870a-770363d88b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates output path for img for widget\n",
    "changeimagename = changeimagename.replace(\".\", \",\")\n",
    "output_path = os.path.join(\"signals/dataset/refined/\", (changeimagename  + \"image.png\"))\n",
    "print(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d626e7-71a5-47cf-b663-64e747015d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates widget information\n",
    "widget = BBoxWidget(\n",
    "    image= output_path,\n",
    "    classes= ['signal'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8e203-847c-4aef-8e28-7396764897b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@widget.on_submit\n",
    "def submit():\n",
    "    \"\"\"\n",
    "    sets up widget\n",
    "    \"\"\"\n",
    "    widget.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046ce70-9441-4dea-92e3-c6f5fb6584b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208a887-bee0-485b-ba6a-2a5e1c42023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget.bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9b33a-818f-43da-95c8-72165aef6ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sigmf dataset\n",
    "filesigmf = output_path[:-4]\n",
    "\n",
    "print(\"sigmf\", filesigmf)\n",
    "signal = sigmffile.fromfile(filesigmf)\n",
    "print(signal)\n",
    "#initialize other global variables\n",
    "img = plt.imread(output_path)\n",
    "height, width = img.shape[:2]\n",
    "edits = edit.split(\",\")\n",
    "nlabels = []\n",
    "labels = []\n",
    "annotations = []\n",
    "ydsimage= yds1[0].img\n",
    "\n",
    "output_path = output_path.replace(\"image_name\", \"\")\n",
    "filemeta =  output_path.replace(\".png\", \".sigmf-meta\")\n",
    "filetxt = output_path.replace(\".png\", \"image.txt\")\n",
    "filetxt = filetxt.replace(\"signals/dataset/refined/\", \"\")\n",
    "filetxt = filetxt.replace(\"imageimage\", \"image\")\n",
    "filename = os.path.join(\"signals/annotated_dataset/labels/\" , changeimagename + \"image.txt\" )\n",
    "#iterates through the new annotations\n",
    "for box in widget.bboxes:\n",
    "    # print(box)\n",
    "    cx = (box['x'] + box['width']//2)/width\n",
    "    cy = (box['y'] + box['height']//2)/height\n",
    "    sx = cx - box['width']// 2\n",
    "    ex = cx + box['width'] // 2\n",
    "    new_width = box['width']/width\n",
    "    new_height = box['height']/height\n",
    "    duration = ex - sx\n",
    "    fs = sample_rate\n",
    "    top_freq = (float(fs)/2.0)-((box['y']/NFFT)*float(fs))+cy\n",
    "    bottom_freq = (float(fs)/2.0)-((box['height']/NFFT)*float(fs))+cy\n",
    "    # Retrieve annotations at index z\n",
    "    annotations += [[cy, bottom_freq,top_freq,duration]]\n",
    "    labels += [[cx, cy, new_width, new_height]]\n",
    "# print(labels)\n",
    "\n",
    "\n",
    "#updates meta and yds files\n",
    "print(filetxt)\n",
    "for y in range(len(edits)):\n",
    "    print(y)\n",
    "    an = signal.get_annotations()\n",
    "    # print(an)\n",
    "    if an[y]['core:comment'] == edits[y]:\n",
    "        an[y]['core:sample_start'] == annotations[y][0]\n",
    "        an[y]['core:freq_lower_edge'] = annotations[y][1]\n",
    "        an[y]['core:freq_upper_edge'] = annotations[y][2]\n",
    "        an[y]['core:sample_count'] =  annotations[y][3]\n",
    "        signal.validate()\n",
    "        # print(filemeta)\n",
    "        signal.tofile(filemeta)\n",
    "    # print(filename)\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            item = line.split()  # Automatically splits on whitespace\n",
    "            if item[0] == edit[y]:  # Check if the first element matches edit[y]\n",
    "                # Create the new label list\n",
    "                nlabel = [item[0], labels[y][0], labels[y][1],labels[y][2], labels[y][3]]\n",
    "                if nlabel not in nlabels:\n",
    "                    nlabels.append(nlabel)\n",
    "            else:\n",
    "                nlabels.append(item)  # Append the original item if no match\n",
    "print(nlabels)\n",
    "\n",
    "\n",
    "#updates annotates_dataset txt labels\n",
    "with open(filename, \"w\") as file:\n",
    "    for line in nlabels:\n",
    "        file.write(str(line[0]) + \" \"+ str(line[1]) + \" \"+ str(line[2]) + \" \"+str(line[3])+ \" \"+ str(line[4]) + \"\\n\")\n",
    "\n",
    "# saves and displays updated photo\n",
    "INDEX = 0\n",
    "for x in range (len(yds1)):\n",
    "    imgname = os.listdir(yds1.root_filepath + \"images\")[x]\n",
    "    imgname =imgname[:-3] + \"png\"\n",
    "    print(imagename)\n",
    "    if imgname == imagename:\n",
    "        INDEX = x\n",
    "print(INDEX)\n",
    "\n",
    "image= yds1[INDEX].img\n",
    "boxclass = []\n",
    "labels = nlabels\n",
    "image = 1 - image.numpy()\n",
    "\n",
    "# font\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# fontScale\n",
    "FONTSCALE = 1\n",
    "# Blue color in BGR\n",
    "color = (0, 255, 0)\n",
    "image = (np.stack([image[0,:,:]]*3).transpose(1,2,0)*255).astype(np.uint8)\n",
    "for label in labels:\n",
    "    cid, cx, cy, w, h = label\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    x1 = int((float(cx) - float(w)/2)*img_w)\n",
    "    x2 = int((float(cx) + float(w)/2)*img_w)\n",
    "    y1 = int((float(cy) - float(h)/2)*img_h)\n",
    "    y2 = int((float(cy) + float(h)/2)*img_h)\n",
    "    image = cv2.rectangle(image.copy(), (x1, y1), (x2, y2), color=(255,0,0), thickness=1)\n",
    "    # org\n",
    "    org = (x1, y1)\n",
    "    # Using cv2.putText() method\n",
    "    image = cv2.putText(image.copy(), str(cid) , org, FONT, FONTSCALE, color)\n",
    "\n",
    "plt.imshow(image)\n",
    "if cv2.imwrite(output_path, image):\n",
    "    print(f\"Image saved successfully: {output_path}\")\n",
    "else:\n",
    "    print(\"Failed to save image.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
