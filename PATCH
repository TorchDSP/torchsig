diff --git a/examples/bring_your_own_data_npy_example.ipynb b/examples/bring_your_own_data_npy_example.ipynb
index 49cfc34e9..2f2b2836b 100644
--- a/examples/bring_your_own_data_npy_example.ipynb
+++ b/examples/bring_your_own_data_npy_example.ipynb
@@ -36,9 +36,10 @@
     "import pprint\n",
     "\n",
     "# TorchSig\n",
-    "from torchsig.datasets.datasets import ExternalTorchSigDataset\n",
-    "from torchsig.datasets.dataset_metadata import ExternalDatasetMetadata\n",
-    "from torchsig.utils.file_handlers import ExternalFileHandler\n",
+    "from torchsig.datasets.datasets import StaticTorchSigDataset\n",
+    "from torchsig.signals.signal_types import Signal\n",
+    "from torchsig.datasets.dataset_metadata import DatasetMetadata\n",
+    "from torchsig.utils.file_handlers import FileReader\n",
     "from torchsig.transforms.transforms import ComplexTo2D"
    ]
   },
@@ -167,9 +168,9 @@
    "id": "8869659d-2933-4d72-8de8-318aeae9db3b",
    "metadata": {},
    "source": [
-    "## Step 2. ExternalFileHandler\n",
+    "## Step 2. FileReader\n",
     "\n",
-    "To have your data on disk interface with TorchSig, you must write your own `ExternalFileHandler` so TorchSig knows how to handle your data. Make sure to call `super()`.\n",
+    "To have your data on disk interface with TorchSig, you must write your own `FileReader` so TorchSig knows how to handle your data. Make sure to call `super()`.\n",
     "\n",
     "Note that the metadata must at least have:\n",
     "- `class_name`\n",
@@ -183,7 +184,7 @@
    "metadata": {},
    "outputs": [],
    "source": [
-    "class BYODExampleFileHandler(ExternalFileHandler):\n",
+    "class BYODExampleFileHandler(FileReader):\n",
     "\n",
     "    def __init__(\n",
     "        self,\n",
@@ -191,7 +192,9 @@
     "    ):\n",
     "        super().__init__(root=root)\n",
     "\n",
-    "        self.class_list = ['BPSK', 'QPSK', 'Noise']  \n",
+    "        self.class_list = ['BPSK', 'QPSK', 'Noise'] \n",
+    "        self.dataset_length = None\n",
+    "        self.dataset_metadata = self.load_dataset_metadata()\n",
     "\n",
     "    def size(self) -> int:\n",
     "        try:\n",
@@ -202,13 +205,12 @@
     "        except:\n",
     "            raise ValueError(f\"Error loading {root}/info.json\")\n",
     "    \n",
-    "    def load_dataset_metadata(self) -> ExternalDatasetMetadata:\n",
+    "    def load_dataset_metadata(self):\n",
     "        try:\n",
     "            with open(f\"{self.root}/info.json\", \"r\") as f:\n",
     "                dataset_info = json.load(f)\n",
     "\n",
-    "            return ExternalDatasetMetadata(\n",
-    "                # minimum fields required for ExternalDatasetMetadata\n",
+    "            return DatasetMetadata(\n",
     "                num_iq_samples_dataset = dataset_info[\"num_samples\"],\n",
     "                sample_rate = dataset_info[\"sample_rate\"],\n",
     "                class_list = dataset_info[\"class_labels\"],\n",
@@ -217,7 +219,7 @@
     "        except:\n",
     "            raise ValueError(f\"Error loading {self.root}/info.json\")\n",
     "\n",
-    "    def load(self, idx: int) -> Tuple[np.ndarray, List[Dict]]:\n",
+    "    def read(self, idx: int) -> Tuple[np.ndarray, List[Dict]]:\n",
     "        try:\n",
     "            # loads entire data to access an element: inefficient, but acceptable for a\n",
     "            # small basic example - use memory mapping or another format for better efficiency\n",
@@ -239,14 +241,20 @@
     "\n",
     "                metadata = row\n",
     "\n",
-    "            return data, [metadata]\n",
+    "            return Signal(data=data, metadata=metadata, component_signals=[], dataset_metadata=self.dataset_metadata)\n",
     "        except:\n",
     "            raise ValueError(f\"Error loading {root}/info.json\")\n",
     "\n",
+    "    def __len__(self):\n",
+    "        if self.dataset_length == None:\n",
+    "            self.dataset_length = len(np.load(f\"{self.root}/data.npy\"))\n",
+    "        return self.dataset_length\n",
+    "        \n",
+    "\n",
     "test = BYODExampleFileHandler(root)\n",
     "print(f'Size: {test.size()}')\n",
     "print(f'Metadata: {test.load_dataset_metadata()}')\n",
-    "print(f'Load element 2: {test.load(2)}')"
+    "print(f'Load element 2: {test.read(2)}')"
    ]
   },
   {
@@ -254,9 +262,9 @@
    "id": "7bd0e530-9864-475b-8ef1-eee3df5751e6",
    "metadata": {},
    "source": [
-    "## Step 3: ExternalTorchSigDataset\n",
+    "## Step 3: StaticTorchSigDataset\n",
     "\n",
-    "Use `ExternalTorchSigDataset` and custom file handler (above) to load in data."
+    "Use `StaticTorchSigDataset` and custom file handler (above) to load in data."
    ]
   },
   {
@@ -268,8 +276,9 @@
    "source": [
     "root = 'datasets/byod_npy_example'    \n",
     "\n",
-    "custom_dataset = ExternalTorchSigDataset(\n",
-    "    file_handler = BYODExampleFileHandler(root),\n",
+    "custom_dataset = StaticTorchSigDataset(\n",
+    "    file_handler_class = BYODExampleFileHandler,\n",
+    "    root=root,\n",
     "    target_labels = None\n",
     ")\n",
     "print(f\"Dataset size: {len(custom_dataset)}\")\n",
@@ -279,6 +288,16 @@
     "print(f\"metadata: {[meta.to_dict() for meta in sample.get_full_metadata()]}\")"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "c4c5fe9f-f920-441d-b75c-99e8b80b888c",
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "sample"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
@@ -289,8 +308,9 @@
     "# can apply transforms and metadata transforms\n",
     "root = 'datasets/byod_npy_example'    \n",
     "\n",
-    "custom_dataset_2 = ExternalTorchSigDataset(\n",
-    "    file_handler = BYODExampleFileHandler(root),\n",
+    "custom_dataset_2 = StaticTorchSigDataset(\n",
+    "    file_handler_class = BYODExampleFileHandler,\n",
+    "    root=root,\n",
     "    transforms = [ComplexTo2D()],\n",
     "    target_labels = [\"modcod\"]\n",
     ")\n",
@@ -308,14 +328,6 @@
    "metadata": {},
    "outputs": [],
    "source": []
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "id": "d49d2518-4ae1-4fa2-922d-54129cd4bc6f",
-   "metadata": {},
-   "outputs": [],
-   "source": []
   }
  ],
  "metadata": {
diff --git a/examples/bring_your_own_data_sigmf_example.ipynb b/examples/bring_your_own_data_sigmf_example.ipynb
index 518bf56a1..9fcf6cc54 100644
--- a/examples/bring_your_own_data_sigmf_example.ipynb
+++ b/examples/bring_your_own_data_sigmf_example.ipynb
@@ -44,9 +44,10 @@
     "from typing import Tuple, Dict, List, Any\n",
     "\n",
     "# TorchSig\n",
-    "from torchsig.datasets.datasets import ExternalTorchSigDataset\n",
-    "from torchsig.datasets.dataset_metadata import ExternalDatasetMetadata\n",
-    "from torchsig.utils.file_handlers import ExternalFileHandler\n",
+    "from torchsig.datasets.datasets import StaticTorchSigDataset\n",
+    "from torchsig.datasets.dataset_metadata import DatasetMetadata\n",
+    "from torchsig.signals.signal_types import Signal, SignalMetadata\n",
+    "from torchsig.utils.file_handlers import FileReader\n",
     "from torchsig.transforms.transforms import ComplexTo2D"
    ]
   },
@@ -226,9 +227,9 @@
    "id": "8869659d-2933-4d72-8de8-318aeae9db3b",
    "metadata": {},
    "source": [
-    "## Step 2. ExternalFileHandler\n",
+    "## Step 2. FileReader\n",
     "\n",
-    "To have your data on disk interface with TorchSig, you must write your own `ExternalFileHandler` so TorchSig knows how to handle your data. Make sure to call `super()`.\n",
+    "To have your data on disk interface with TorchSig, you must write your own `FileReader` so TorchSig knows how to handle your data. Make sure to call `super()`.\n",
     "\n",
     "Note that the metadata must at least have:\n",
     "- `class_name`\n",
@@ -242,7 +243,7 @@
    "metadata": {},
    "outputs": [],
    "source": [
-    "class BYODExampleFileHandler(ExternalFileHandler):\n",
+    "class BYODExampleFileHandler(FileReader):\n",
     "\n",
     "    def __init__(\n",
     "        self,\n",
@@ -254,9 +255,10 @@
     "        self.meta_filename = f'{root}/byod.sigmf-meta'\n",
     "        self.data_size = None\n",
     "        self.class_list = ['BPSK', 'QPSK', 'Noise'] \n",
+    "        self.dataset_metadata = self.load_dataset_metadata()\n",
     "\n",
     "\n",
-    "    def size(self) -> int:\n",
+    "    def __len__(self) -> int:\n",
     "        if self.data_size is None:\n",
     "            try:\n",
     "                loaded_sigmf = sigmffile.fromfile(self.meta_filename)\n",
@@ -267,7 +269,7 @@
     "        return self.data_size\n",
     "\n",
     "    \n",
-    "    def load_dataset_metadata(self) -> ExternalDatasetMetadata:\n",
+    "    def load_dataset_metadata(self) -> DatasetMetadata:\n",
     "        try:\n",
     "            loaded_sigmf = sigmffile.fromfile(self.meta_filename)\n",
     "            num_iq_samples_dataset = loaded_sigmf.get_global_field('core:signal_length')\n",
@@ -276,8 +278,7 @@
     "            num_samples = loaded_sigmf.get_global_field('core:signal_count')\n",
     "\n",
     "            \n",
-    "            return ExternalDatasetMetadata(\n",
-    "                # minimum fields required for ExternalDatasetMetadata\n",
+    "            return DatasetMetadata(\n",
     "                num_iq_samples_dataset = num_iq_samples_dataset,\n",
     "                sample_rate = sample_rate,\n",
     "                class_list = class_list,\n",
@@ -287,7 +288,7 @@
     "            raise ValueError(f\"Error loading {self.meta_filename}\")\n",
     "\n",
     "\n",
-    "    def load(self, idx: int) -> Tuple[np.ndarray, List[Dict]]:\n",
+    "    def read(self, idx: int) -> Tuple[np.ndarray, List[Dict]]:\n",
     "        try:\n",
     "            sigmf_file = sigmffile.fromfile(self.meta_filename)   # creates data memory map access\n",
     "            sample_rate = sigmf_file.get_global_field(SigMFFile.SAMPLE_RATE_KEY)\n",
@@ -305,16 +306,16 @@
     "            stop_idx = start_idx + sigmf_signal_meta['core:sample_count'] - 1\n",
     "            data = sigmf_file[start_idx:stop_idx]\n",
     "\n",
-    "            return data, [meta]\n",
+    "            return Signal(data, metadata=meta, component_signals=[], dataset_metadata=self.dataset_metadata)\n",
     "        \n",
     "        except:\n",
     "            raise ValueError(f\"Error loading {self.meta_filename}\")            \n",
     "\n",
     "\n",
     "test = BYODExampleFileHandler(root)\n",
-    "print(f'Size: {test.size()}')\n",
+    "print(f'Size: {len(test)}')\n",
     "print(f'Metadata: {test.load_dataset_metadata()}')\n",
-    "print(f'Load element 2: {test.load(2)}')"
+    "print(f'Load element 2: {test.read(2)}')"
    ]
   },
   {
@@ -322,9 +323,9 @@
    "id": "7bd0e530-9864-475b-8ef1-eee3df5751e6",
    "metadata": {},
    "source": [
-    "## Step 3: ExternalTorchSigDataset\n",
+    "## Step 3: StaticTorchSigDataset\n",
     "\n",
-    "Use `ExternalTorchSigDataset` and custom file handler (above) to load in data."
+    "Use `StaticTorchSigDataset` and custom file handler (above) to load in data."
    ]
   },
   {
@@ -336,8 +337,9 @@
    "source": [
     "root = 'datasets/byod_sigmf_example'    \n",
     "\n",
-    "custom_dataset = ExternalTorchSigDataset(\n",
-    "    file_handler = BYODExampleFileHandler(root),\n",
+    "custom_dataset = StaticTorchSigDataset(\n",
+    "    root=root,\n",
+    "    file_handler_class = BYODExampleFileHandler,\n",
     "    target_labels = None\n",
     ")\n",
     "print(f\"Dataset size: {len(custom_dataset)}\")\n",
@@ -357,8 +359,9 @@
     "# can apply transforms and metadata transforms\n",
     "root = 'datasets/byod_sigmf_example'    \n",
     "\n",
-    "custom_dataset_2 = ExternalTorchSigDataset(\n",
-    "    file_handler = BYODExampleFileHandler(root),\n",
+    "custom_dataset_2 = StaticTorchSigDataset(\n",
+    "    root=root,\n",
+    "    file_handler_class = BYODExampleFileHandler,\n",
     "    transforms = [ComplexTo2D()],\n",
     "    target_labels = [\"modcod\"]\n",
     ")\n",
@@ -368,6 +371,14 @@
     "print(f\"data: {data.shape}\")\n",
     "print(f\"metadata: {metadata}\")"
    ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "id": "4c7f853c-d45a-4a6f-b488-c6e9eb2e36b8",
+   "metadata": {},
+   "outputs": [],
+   "source": []
   }
  ],
  "metadata": {
diff --git a/tests/datasets/test_datasets.py b/tests/datasets/test_datasets.py
index 26a719b6f..2f6d88f30 100644
--- a/tests/datasets/test_datasets.py
+++ b/tests/datasets/test_datasets.py
@@ -1,15 +1,15 @@
 """Unit Tests for datasets
 """
-from torchsig.datasets.dataset_metadata import DatasetMetadata, ExternalDatasetMetadata
-from torchsig.datasets.datasets import TorchSigIterableDataset, StaticTorchSigDataset, ExternalTorchSigDataset
+from torchsig.datasets.datasets import TorchSigIterableDataset, StaticTorchSigDataset
 from torchsig.utils.writer import DatasetCreator, default_collate_fn
 from torchsig.signals.signal_lists import TorchSigSignalLists
 from torchsig.transforms.metadata_transforms import YOLOLabel
 from torchsig.utils.data_loading import WorkerSeedingDataLoader
 from torchsig.transforms.transforms import Spectrogram, ComplexTo2D
 from torchsig.signals.signal_types import Signal
-from torchsig.utils.file_handlers import ExternalFileHandler
 from torchsig.utils.dsp import TorchSigRealDataType
+from torchsig.utils.file_handlers import FileReader
+from torchsig.datasets.dataset_metadata import DatasetMetadata
 
 # Third Party
 import numpy as np
@@ -37,7 +37,6 @@ RTOL = 1E-6
 wb_data_dir =  Path.joinpath(Path(__file__).parent,'datasets/dataset_data')
 wb_image_dir = Path.joinpath(Path(__file__).parent,'datasets/dataset_images')
 getitem_dir = Path.joinpath(Path(__file__).parent,'datasets/getitem_data')
-external_dir = Path.joinpath(Path(__file__).parent,'datasets/external_data')
 
 # directory for test data
 def setup_module(module):
@@ -189,216 +188,6 @@ def test_StaticDataset_getitem(num_signals_max: int, target_labels):
 
         # verify_getitem_targets(num_signals_max, target_labels, sample)
 
-
-class BYODExampleFileHandler(ExternalFileHandler):
-
-    def __init__(
-        self,
-        root: str
-    ):
-        super().__init__(root=root)
-
-        self.class_list = ['BPSK', 'QPSK', 'Noise']  
-
-    def size(self) -> int:
-        try:
-            with open(f"{self.root}/info.json", "r") as f:
-                dataset_info = json.load(f)
-
-            return dataset_info["size"]
-        except:
-            raise ValueError(f"Error loading {root}/info.json")
-    
-    def load_dataset_metadata(self) -> ExternalDatasetMetadata:
-        try:
-            with open(f"{self.root}/info.json", "r") as f:
-                dataset_info = json.load(f)
-
-            return ExternalDatasetMetadata(
-                # minimum fields required for ExternalDatasetMetadata
-                num_iq_samples_dataset = dataset_info["dataset_length"],
-                sample_rate = dataset_info["sample_rate"],
-                class_list = dataset_info["class_labels"],
-                dataset_length = dataset_info["size"]
-            )
-        except:
-            raise ValueError(f"Error loading {self.root}/info.json")
-
-    def load(self, idx: int) -> Tuple[np.ndarray, List[Dict]]:
-        try:
-            # load data
-            data = np.load(f"{self.root}/data.npy")[idx]
-
-            with open(f"{self.root}/metadata.csv", "r") as f:
-                reader = csv.DictReader(f, fieldnames=["index", "label", "modcod", "sample_rate"])
-                # get to idx row
-                row = next(itertools.islice(reader, idx, idx+1), None)
-                if row is None:
-                    raise IndexError(f"Metadata idx {idx} is out of bounds")
-
-                row["index"] = int(row["index"])
-                row["sample_rate"] = float(row["sample_rate"])
-                # add class_name
-                row["class_name"] = row["label"].lower()
-                # add class index
-                row["class_index"] = self.class_list.index(row["label"])
-
-                metadata = row
-
-            return data, [metadata]
-        except:
-            raise ValueError(f"Error loading {root}/info.json")
-
-
-class BYODExampleFileHandler(ExternalFileHandler):
-
-    def __init__(
-        self,
-        root: str
-    ):
-        super().__init__(root=root)
-
-        self.class_list = ['BPSK', 'QPSK', 'Noise']  
-
-    def size(self) -> int:
-        try:
-            with open(f"{self.root}/info.json", "r") as f:
-                dataset_info = json.load(f)
-
-            return dataset_info["dataset_length"]
-        except:
-            raise ValueError(f"Error loading {root}/info.json")
-    
-    def load_dataset_metadata(self) -> ExternalDatasetMetadata:
-        try:
-            with open(f"{self.root}/info.json", "r") as f:
-                dataset_info = json.load(f)
-
-            return ExternalDatasetMetadata(
-                # minimum fields required for ExternalDatasetMetadata
-                num_iq_samples_dataset = dataset_info["dataset_length"],
-                sample_rate = dataset_info["sample_rate"],
-                class_list = dataset_info["class_labels"],
-            )
-        except:
-            raise ValueError(f"Error loading {self.root}/info.json")
-
-    def load(self, idx: int) -> Tuple[np.ndarray, List[Dict]]:
-        try:
-            # load data
-            data = np.load(f"{self.root}/data.npy")[idx]
-
-            with open(f"{self.root}/metadata.csv", "r") as f:
-                reader = csv.DictReader(f, fieldnames=["index", "label", "modcod", "sample_rate"])
-                # get to idx row
-                row = next(itertools.islice(reader, idx, idx+1), None)
-                if row is None:
-                    raise IndexError(f"Metadata idx {idx} is out of bounds")
-
-                row["index"] = int(row["index"])
-                row["sample_rate"] = float(row["sample_rate"])
-                # add class_name
-                row["class_name"] = row["label"].lower()
-                # add class index
-                row["class_index"] = self.class_list.index(row["label"])
-
-                metadata = row
-
-            return data, [metadata]
-        except:
-            raise ValueError(f"Error loading {root}/info.json")
-
-def test_ExternalTorchSigDataset() -> None:
-    root = external_dir               # data file top-level folder 
-    seed = 1234567890                         # rng seed
-
-    # directories
-    os.makedirs(root, exist_ok=True)
-
-    # Parameters
-    fs = 1_000_000                              # 1 MHz sample-rate (fixed rate)
-    dataset_length = 1024                          # samples per data (fixed size)
-    dataset_size = 8                            # dataset size
-    labels = ['BPSK', 'QPSK', 'Noise']          # three arbitrary metadata class labels (strings)
-    modcod = [0, 1, 2]                          # three arbitrary metadata integers
-    rng = np.random.default_rng(seed)           # random number generator
-
-    # Create user's external data: non-TorchSig synthetic data along with metadata
-    signals_array = np.empty((dataset_size, dataset_length), dtype=np.complex64)  # store all data in memory
-    meta_rows = []                                           # store all metadata in memory
-
-    t = np.arange(dataset_length) / fs  # timesteps
-
-    # create dataset
-    for idx in range(dataset_size):
-        label = rng.choice(labels)
-        mc = rng.choice(modcod)
-        
-        if label == "BPSK":
-            bits   = rng.integers(0, 2, dataset_length)
-            sig    = (2*bits-1) + 0j
-        elif label == "QPSK":
-            bits   = rng.integers(0, 4, dataset_length)
-            table  = {0:1+1j, 1:1-1j, 2:-1+1j, 3:-1-1j}
-            sig    = np.vectorize(table.get)(bits)
-        else:  # white noise
-            sig = (rng.normal(size=dataset_length) + 1j*rng.normal(size=dataset_length)) * 0.1
-        
-        sig /= np.sqrt((np.abs(sig)**2).mean()) # normalize power for consistency
-        signals_array[idx] = sig.astype(np.complex64)
-        
-        # add to metadata
-        meta_rows.append(
-            dict(
-                index=idx, 
-                label=label, 
-                modcod=mc, 
-                sample_rate=fs
-            )
-        )
-
-    # write information about dataset
-    global_metadata = {
-        "size": dataset_size,
-        "dataset_length": dataset_length,
-        "class_labels": labels,
-        "sample_rate": fs
-    }
-    with open(f"{root}/info.json", 'w') as f:
-        json.dump(global_metadata, f, indent=4)
-
-    # write data as npy
-    np.save(f"{root}/data.npy", signals_array)
-
-    # write metadata
-    with open(f"{root}/metadata.csv", 'w', newline='') as f:
-        csv.DictWriter(f, fieldnames=meta_rows[0].keys()).writerows(meta_rows)
-
-    print(f"Synthetic signals + metadata staged in {root}")
-
-    custom_dataset = ExternalTorchSigDataset(
-        file_handler = BYODExampleFileHandler(root),
-        target_labels = None
-    )
-    print(f"Dataset size: {len(custom_dataset)}")
-
-    sample = custom_dataset[4]
-    print(f"data: {sample.data}")
-    print(f"metadata: {[meta.to_dict() for meta in sample.get_full_metadata()]}")
-
-    custom_dataset_2 = ExternalTorchSigDataset(
-        file_handler = BYODExampleFileHandler(root),
-        transforms = [ComplexTo2D()],
-        target_labels = ["modcod"]
-    )
-    print(f"Dataset size: {len(custom_dataset_2)}")
-
-    data, metadata = custom_dataset_2[4]
-    print(f"data: {data.shape}")
-    print(f"metadata: {metadata}")
-
-
-
 @pytest.mark.parametrize("params, is_error", [
     (
         {'dataset_length': 10},
diff --git a/tests/signals/test_signal_types.py b/tests/signals/test_signal_types.py
index 58ede6f2f..67e0b546d 100644
--- a/tests/signals/test_signal_types.py
+++ b/tests/signals/test_signal_types.py
@@ -69,7 +69,6 @@ def create_verify_signal_metadata(
         class_name,
         class_index
     )
-    m.verify()
 
 good_signal_metadata = dict(
     center_freq = 0.0,
@@ -198,66 +197,6 @@ def test_valid_SignalMetadata(
             run_SignalMetadata_test(**signal_metadata)
 
 
-def test_invalid_SignalMetadata():
-    dataset_metadata = get_metadata()
-
-    bad_params = dict(
-        # dataset_metadata
-        dataset_metadata = [None],
-        # center_freq
-        center_freq = [
-            dataset_metadata.signal_center_freq_min, 
-            -99999, 
-            dataset_metadata.signal_center_freq_max + 1
-        ],
-        # bandwidth
-        bandwidth = [
-            0.0, 
-            dataset_metadata.sample_rate + 1
-        ],
-        # start_in_samples
-        start_in_samples = [
-            -1,
-            dataset_metadata.num_iq_samples_dataset,
-            dataset_metadata.num_iq_samples_dataset + 1
-        ],
-        # duration_in_samples
-        duration_in_samples = [
-            0,
-            dataset_metadata.num_iq_samples_dataset + 1,
-        ],
-        # snr_db
-        snr_db = [
-            -1,
-            -0.1,
-        ],
-        # class_name
-        class_name = [
-            None,
-            True,
-            7,
-        ],
-        # class_index
-        class_index = [
-            -1,
-            None,
-            3.2,
-            "hi"
-        ]
-    )
-
-    signal_metadata = deepcopy(good_signal_metadata)
-    signal_metadata['dataset_metadata'] = dataset_metadata
-
-    for field in bad_params.keys():
-        for bad_param in bad_params[field]:
-            signal_metadata[field] = bad_param
-
-            signal_metadata['is_error'] = True
-    
-            run_SignalMetadata_test(**signal_metadata)
-
-
 # Signal Tests
 
 @pytest.mark.parametrize("data, is_error", [
@@ -265,14 +204,14 @@ def test_invalid_SignalMetadata():
     (np.zeros((good_signal_metadata['duration_in_samples']), dtype=TorchSigComplexDataType), False),
     (np.random.random((good_signal_metadata['duration_in_samples'])).astype(TorchSigComplexDataType), False),
 
-    (np.ones((good_signal_metadata['duration_in_samples']), dtype=int), True),
-    (np.zeros((good_signal_metadata['duration_in_samples']), dtype=float), True),
-    (np.random.random((good_signal_metadata['duration_in_samples'])).astype(float), True),
+    #(np.ones((good_signal_metadata['duration_in_samples']), dtype=int), True),
+    #(np.zeros((good_signal_metadata['duration_in_samples']), dtype=float), True),
+    #(np.random.random((good_signal_metadata['duration_in_samples'])).astype(float), True),
 
-    (np.ones((good_signal_metadata['duration_in_samples'] + 1), dtype=TorchSigComplexDataType), True),
-    (np.zeros((good_signal_metadata['duration_in_samples'] - 1), dtype=TorchSigComplexDataType), True),
+    #(np.ones((good_signal_metadata['duration_in_samples'] + 1), dtype=TorchSigComplexDataType), True),
+    #(np.zeros((good_signal_metadata['duration_in_samples'] - 1), dtype=TorchSigComplexDataType), True),
 
-    (np.random.random((good_signal_metadata['duration_in_samples'] + 2)).astype(float), True),
+    #(np.random.random((good_signal_metadata['duration_in_samples'] + 2)).astype(float), True),
 ])
 def test_Signal(data: np.ndarray, is_error: bool):
 
@@ -285,13 +224,11 @@ def test_Signal(data: np.ndarray, is_error: bool):
                 data = data,
                 metadata = SignalMetadata(**signal_metadata)
             )
-            s.verify()
     else:
         s = Signal(
             data = data,
             metadata = SignalMetadata(**signal_metadata)
         )
-        s.verify()
 
 # Signal Tests
 good_signal_metadata = dict(
diff --git a/tests/test_utils.py b/tests/test_utils.py
index d3bfe76c2..a1f7bfd31 100644
--- a/tests/test_utils.py
+++ b/tests/test_utils.py
@@ -4,8 +4,6 @@ import matplotlib.pyplot as plt
 
 from typing import List, Tuple, Any
 
-from torchsig.datasets.dataset_metadata import ExternalDatasetMetadata
-
 # Signal Metadata type checking
 
 signal_metadata_floats = [
diff --git a/torchsig/datasets/__init__.py b/torchsig/datasets/__init__.py
index 206c74e55..2657dbebf 100755
--- a/torchsig/datasets/__init__.py
+++ b/torchsig/datasets/__init__.py
@@ -1,12 +1,10 @@
 """TorchSig datasets
 """
-from .dataset_metadata import DatasetMetadata, ExternalDatasetMetadata
-from .datasets import TorchSigIterableDataset, StaticTorchSigDataset, ExternalTorchSigDataset
+from .dataset_metadata import DatasetMetadata
+from .datasets import TorchSigIterableDataset, StaticTorchSigDataset
 
 __all__ = [
     "DatasetMetadata",
-    "ExternalDatasetMetadata",
     "TorchSigIterableDataset",
-    "StaticTorchSigDataset",
-    "ExternalTorchSigDataset"
+    "StaticTorchSigDataset"
 ]
\ No newline at end of file
diff --git a/torchsig/datasets/dataset_metadata.py b/torchsig/datasets/dataset_metadata.py
index 463628f5e..9128feb7e 100644
--- a/torchsig/datasets/dataset_metadata.py
+++ b/torchsig/datasets/dataset_metadata.py
@@ -5,12 +5,6 @@ from __future__ import annotations
 
 # TorchSig
 from torchsig.signals.signal_lists import TorchSigSignalLists
-from torchsig.utils.verify import (
-    verify_int,
-    verify_float,
-    verify_distribution_list,
-    verify_list
-)
 from torchsig.utils.printing import (
     dataset_metadata_str,
 )
@@ -45,8 +39,6 @@ class DatasetMetadata():
 
     This class stores metadata related to the dataset, including parameters
     related to signal generation, transforms, dataset path, and sample distribution.
-    It also handles the verification of dataset settings and ensures that the configuration 
-    is valid for the dataset creation process.
     """
 
     minimum_params: List[str] = [
@@ -57,8 +49,8 @@ class DatasetMetadata():
 
     def __init__(
         self, 
-        num_iq_samples_dataset: int, 
-        fft_size: int,
+        num_iq_samples_dataset: int = 0, 
+        fft_size: int = 0,
         num_signals_min: int = 1,
         num_signals_max: int = 1,
         sample_rate: float = 10e6,
@@ -135,9 +127,6 @@ class DatasetMetadata():
         # provide a noise power reference in dB
         self.noise_power_db = 0
 
-        # run _verify() to ensure metadata is valid
-        self.verify()
-
     def update_from(self, attr_dict):
         """updates the fields of this metadata object with the values in attr_dict; good for joining metadata together
             modifies existing object, and return without copying
@@ -146,178 +135,6 @@ class DatasetMetadata():
             setattr(self, key, attr_dict[key])
         return self
 
-    def verify(self) -> None:
-        """Verify that metadata is valid.
-
-        This method checks the configuration of the metadata, ensuring all parameters
-        are consistent, valid, and appropriate for dataset creation. It will raise 
-        ValueError if any configuration is found to be incorrect.
-
-        Raises:
-            ValueError: If any dataset configuration is invalid.
-        """
-
-        self.class_distribution = verify_distribution_list(
-            self.class_distribution, 
-            len(self.class_list), 
-            "class_distribution", 
-            "class_list"
-        )
-        self.num_signals_distribution = verify_distribution_list(
-            self.num_signals_distribution, 
-            len(self.num_signals_range), 
-            "num_signals_distribution", 
-            "[num_signals_min, num_signals_max]"
-        )
-
-        self.class_list = verify_list(self.class_list, "class_list")
-
-        # check all of the input parameters
-        self.sample_rate = verify_float(
-            self.sample_rate,
-            name = "sample_rate",
-            low = 0.0,
-            exclude_low = True
-        )
-
-        self.fft_size = verify_int(
-            self.fft_size,
-            name = "fft_size",
-            low = 0,
-            exclude_low = True
-        )
-
-        self.fft_stride = verify_int(
-            self.fft_stride,
-            name = "fft_stride",
-            low = 0,
-            high = self.fft_size,
-            exclude_low = True,
-        )
-
-        
-        self.num_iq_samples_dataset = verify_int(
-            self.num_iq_samples_dataset,
-            name = "num_iq_samples_dataset",
-            low = 0,
-            exclude_low = True
-        )
-
-        self.num_signals_max = verify_int(
-            self.num_signals_max,
-            name = "num_signals_max",
-            low = 0
-        )
-
-        self.num_signals_min = verify_int(
-            self.num_signals_min,
-            name = "num_signals_min",
-            low = 0,
-            high = self.num_signals_max
-        )
-
-        self.snr_db_max = verify_float(
-            self.snr_db_max,
-            name = "snr_db_max",
-            low = None,
-        )
-
-        self.snr_db_min = verify_float(
-            self.snr_db_min,
-            name = "snr_db_min",
-            low = None,
-            high = self.snr_db_max
-        )
-
-        self.signal_duration_max = verify_float(
-            self.signal_duration_max,
-            name = "signal_duration_max",
-            low = self.dataset_duration_min,
-            high = self.dataset_duration_max
-        )
-
-        self.signal_duration_min = verify_float(
-            self.signal_duration_min,
-            name = "signal_duration_min",
-            low = self.dataset_duration_min,
-            high = self.dataset_duration_max
-        )
-
-        self.signal_bandwidth_min = verify_float(
-            self.signal_bandwidth_min,
-            name = "signal_bandwidth_min",
-            low = self.dataset_bandwidth_min,
-            high = self.dataset_bandwidth_max
-        )
-
-        self.signal_bandwidth_max = verify_float(
-            self.signal_bandwidth_max,
-            name = "signal_bandwidth_max",
-            low = self.dataset_bandwidth_min,
-            high = self.dataset_bandwidth_max
-        )
-
-        self.signal_center_freq_min = verify_float(
-            self.signal_center_freq_min,
-            name = "signal_center_freq_min",
-            low = self.dataset_center_freq_min,
-            high = self.dataset_center_freq_max
-        )
-
-        self.signal_center_freq_max = verify_float(
-            self.signal_center_freq_max,
-            name = "signal_center_freq_max",
-            low = self.dataset_center_freq_min,
-            high = self.dataset_center_freq_max
-        )
-
-        self.cochannel_overlap_probability = verify_float(
-            self.cochannel_overlap_probability,
-            name = "cochannel_overlap_probability",
-            low = 0,
-            high = 1
-        )
-
-        # check derived values
-        
-        verify_float(
-            self.fft_frequency_resolution,
-            name = "fft_frequency_resolution",
-            low = 0.0,
-            exclude_low = True
-        )
-
-
-        verify_float(
-            self.fft_frequency_max,
-            name = "fft_frequency_max",
-            low = None,
-            high = None,
-        )
-
-        verify_float(
-            self.fft_frequency_min,
-            name = "fft_frequency_max",
-            low = None,
-            high = self.fft_frequency_max,
-            exclude_high = True
-        )
-
-        verify_int(
-            self.signal_duration_in_samples_max,
-            name = "signal_duration_in_samples_max",
-            low = self.dataset_duration_in_samples_min,
-            exclude_low = True
-        )
-
-        verify_int(
-            self.signal_duration_in_samples_min,
-            name = "signal_duration_in_samples_min",
-            low = 0,
-            high = self.dataset_duration_in_samples_max,
-            exclude_low = True
-        )
-
     def __str__(self) -> str:
         return dataset_metadata_str(self)
 
@@ -605,105 +422,3 @@ class DatasetMetadata():
         """
         epsilon = 1e-10
         return (self.sample_rate/2)*(1-epsilon)
-
-
-class ExternalDatasetMetadata():
-    """Dataset Metadata class for external data, with less required infrastructure
-    and fields than the internal metadata class that generates TorchSig datasets.
-    """
-
-    minimum_params: List[str] = [
-        'num_iq_samples_dataset',
-        'class_list',
-        'sample_rate'
-    ]
-
-    def __init__(
-        self, 
-        num_iq_samples_dataset: int,
-        class_list: List[str] = [],
-        sample_rate: float = 10e6,
-        **kwargs
-    ):
-        """Initializes ExternalDatasetMetadata.
-
-        Args:
-            num_iq_samples_dataset (int): Length of I/Q array in dataset.
-            class_list (List[str], optional): Signal class name list. Defaults to []].
-            sample_rate (float, optional): Sample rate for dataset. Defaults to 10e6.
-        Raises:
-            ValueError: If any of the provided parameters are invalid or incompatible.
-        """            
-        self.num_iq_samples_dataset = num_iq_samples_dataset
-        self.sample_rate = sample_rate
-        self.class_list = class_list
-        self.kwargs = kwargs
-
-        # run _verify() to ensure metadata is valid
-        self.verify()
-
-    def verify(self) -> None:
-        """Verify that metadata is valid.
-
-        Raises:
-            ValueError: If any dataset configuration is invalid.
-        """
-
-        # check all of the input parameters
-        self.num_iq_samples_dataset = verify_int(
-            self.num_iq_samples_dataset,
-            name = "num_iq_samples_dataset",
-            low = 0,
-            exclude_low = True
-        )
-
-    def __str__(self) -> str:
-        return f"{self.__class__.__name__}"
-
-    def __repr__(self) -> str:
-        """Returns a string representation of the DatasetMetadata instance.
-        
-        This provides a concise summary of the key parameters such as `num_iq_samples_dataset`, 
-        `sample_rate`, `num_signals_max`, and `fft_size`.
-        
-        Returns:
-            str: String representation of the DatasetMetadata instance.
-        """
-        return f"{self.__class__.__name__}(num_iq_samples_dataset={self.num_iq_samples_dataset}, sample_rate={self.sample_rate})"
-    
-    def to_dict(self) -> Dict[str, Any]:
-        """Converts the dataset metadata into a dictionary format.
-
-        This method organizes various metadata fields related to the dataset into categories such as 
-        general dataset information, signal generation parameters, and dataset writing information.
-
-        Returns:
-            Dict[str, Any]: A dictionary representation of the dataset metadata.
-        """
-        # organize fields by area
-
-        required = {
-            'num_iq_samples_dataset': self.num_iq_samples_dataset,
-
-        }
-
-        overrides = {
-            'sample_rate': self.sample_rate,
-            'class_list': deepcopy(self.class_list),
-        }
-
-        # dataset information
-        dataset_info = {
-            'num_iq_samples_dataset': self.num_iq_samples_dataset,
-            'sample_rate': self.sample_rate,
-        }
-
-        read_only = {
-            'info': dataset_info
-        }
-
-        return {
-            'required': required,
-            'overrides': overrides,
-            'read_only': read_only
-        }
diff --git a/torchsig/datasets/dataset_utils.py b/torchsig/datasets/dataset_utils.py
index f805bbcfe..cee8d4737 100644
--- a/torchsig/datasets/dataset_utils.py
+++ b/torchsig/datasets/dataset_utils.py
@@ -75,10 +75,6 @@ def frequency_shift_signal(
     # center frequency is now set, and therefore can be verified
     signal.metadata.center_freq_set = True
 
-    # because we have altered both the IQ samples and metdata, run verify()
-    # to ensure nothing is broken
-    signal.verify()
-
     return signal
 
 
diff --git a/torchsig/datasets/datasets.py b/torchsig/datasets/datasets.py
index 3996a0c80..5ccd10480 100644
--- a/torchsig/datasets/datasets.py
+++ b/torchsig/datasets/datasets.py
@@ -9,7 +9,7 @@ from torchsig.datasets.dataset_utils import (
     to_dataset_metadata, 
     frequency_shift_signal
 )
-from torchsig.signals.signal_types import Signal, SignalMetadataExternal
+from torchsig.signals.signal_types import Signal
 from torchsig.signals.builder import SignalBuilder
 import torchsig.signals.builders as signal_builders
 from torchsig.transforms.base_transforms import Transform
@@ -435,102 +435,3 @@ class StaticTorchSigDataset(Dataset, Seedable):
             f"file_handler_class={self.reader}"
         )
 
-
-
-
-
-class ExternalTorchSigDataset(Dataset):
-    """
-    Lightweight static dataset for importing external (not TorchSig generated) data and metadata from files.
-    
-    Args:
-        root (str): The root directory where the dataset is stored.
-        file_handler_class (ExternalFileHandler): Class used for reading dataset.
-        transforms (list, optional): Transforms to apply to the data (default: []).
-        target_transforms (list, optional): Target transforms to apply (default: []).        
-        
-    """
-    def __init__(
-        self, 
-        file_handler: ExternalFileHandler,
-        transforms: List[Transform] = [],  
-        target_labels: List[str] = []             
-    ):
-        self.transforms = transforms
-        self.target_labels = target_labels
-        self.file_handler = file_handler
-        self.dataset_length = self.file_handler.size()
-        self.dataset_metadata = self.file_handler.load_dataset_metadata()
-        self._verify()
-
-    
-    def _verify(self):
-        # Transforms
-        self.transforms = verify_transforms(self.transforms)   
-
-    
-    def __len__(self) -> int: 
-        return self.dataset_length
-            
-    def __getitem__(self, idx: int) -> Tuple[np.ndarray, Any]:
-        """
-        Retrieves a sample from the static dataset by index.
-
-        Args:
-            idx: sample index.
-
-        Returns:
-            Tuple[data, targets] returned data array and metadata.
-        """
-        if 0 <= idx < len(self):
-            data, signal_metadatas = self.file_handler.load(idx)
-            component_signals = []
-            for signal_metadata in signal_metadatas:
-                if not isinstance(signal_metadata, dict):
-                    raise ValueError(f"Signal metadata is not a dict: {type(signal_metadata)}.")
-                # create external signal metadata
-                esm = SignalMetadataExternal(
-                    self.dataset_metadata,
-                    **signal_metadata
-                )
-                # create component signal
-                component_signal = Signal(
-                    data = np.array([]),
-                    metadata = esm,
-                )
-                # add to component signals
-                component_signals.append(component_signal)
-            
-            # create Signal from component signals
-            sample = Signal(
-                data = data,
-                component_signals = component_signals
-            )
-
-            # apply user transforms
-            for transform in self.transforms:
-                sample = transform(sample)
-
-            # apply metadata transforms
-            # just return data if target_labels is None or empty list
-            if self.target_labels is None:
-                return sample
-            if len(self.target_labels) < 1:
-                return sample.data
-
-            metadatas = sample.get_full_metadata()
-            targets = []
-            if len(self.target_labels) == 1:
-                # just 1 target label
-                # set targets to single item
-                targets = [getattr(metadata, self.target_labels[0]) for metadata in metadatas]
-            else:
-                # multiple target labels
-                for metadata in metadatas:
-                    # for each signal metadata
-                    # apply all target labels
-                    targets += [[getattr(metadata, target_label) for target_label in self.target_labels]]
-
-            return sample.data, targets
-            
-        raise IndexError(f"Index {idx} is out of bounds. Must be [0, {self.__len__()}]")          
\ No newline at end of file
diff --git a/torchsig/signals/__init__.py b/torchsig/signals/__init__.py
index ceea04a0e..76b2a3a23 100644
--- a/torchsig/signals/__init__.py
+++ b/torchsig/signals/__init__.py
@@ -1,6 +1,6 @@
 """ TorchSig Signals
 """
-from .signal_types import Signal, SignalMetadata, SignalMetadataExternal
+from .signal_types import Signal, SignalMetadata
 from .signal_lists import TorchSigSignalLists
 
 __all__ = [
diff --git a/torchsig/signals/builder.py b/torchsig/signals/builder.py
index c8b88e6e0..bbcc6a93b 100644
--- a/torchsig/signals/builder.py
+++ b/torchsig/signals/builder.py
@@ -151,17 +151,12 @@ class SignalBuilder(Builder, Seedable):
         power of the signal based on the PSD estimate to have the appropriate
         SNR and then estimates the total occupied bandwidth of the signal,
         including sidelobes, to develop a more accurate bounding box.
-        `.verify()` ensures that the metadata values are within the appropriate
-        bounds. The Signal() object is then stored for the return call, and
-        `reset()` is called to reset the signal metadata fields to their dataset
-        metadata defaults.
 
         In order, calls:
         - `reset()`
         - `_update_metadata()`
         - `_update_data()`
         - `_correct_bandwidth_and_snr()`
-        - `_signal.verify()`
 
 
         Returns:
@@ -185,9 +180,6 @@ class SignalBuilder(Builder, Seedable):
         # in the frequency domain
         self._correct_bandwith_and_snr()
 
-        # checks that signal and metadata set properly
-        self._signal.verify()
-
         # signal object to be returned
         new_signal = self._signal
 
@@ -400,8 +392,6 @@ class SignalBuilder(Builder, Seedable):
 #         for b in self.builders:
 #             self._signal.signals.append(b.build())
 
-#         self._signal.verify()
-
 #         return self._signal
 
     
diff --git a/torchsig/signals/signal_types.py b/torchsig/signals/signal_types.py
index d0830ecbf..4d24b342f 100644
--- a/torchsig/signals/signal_types.py
+++ b/torchsig/signals/signal_types.py
@@ -21,13 +21,6 @@ from torchsig.utils.dsp import (
     bandwidth_from_lower_upper_freq,
     TorchSigComplexDataType
 )
-from torchsig.utils.verify import (
-    verify_int,
-    verify_float,
-    verify_str,
-    verify_numpy_array,
-    # verify_dict
-)
 
 # Third Party
 import numpy as np
@@ -122,15 +115,6 @@ class SignalMetadata():
         for key in kwargs.keys():
             setattr(self, key, kwargs[key])
 
-    @property
-    def sample_rate(self) -> float:
-        """Signal sample rate
-
-        Returns:
-            float: sample rate
-        """
-        return self.dataset_metadata.sample_rate
-
     @property
     def start(self) -> float:
         """Signal start normalized to duration of signal
@@ -321,152 +305,23 @@ class SignalMetadata():
         """        
         return copy.deepcopy(self)
 
-
-    def verify(self) -> None:
-        """Verifies Signal Metadata fields
-
-        Raises:
-            MissingSignalMetadata: Metadata missing.
-            InvalidSignalMetadata: Metadata invalid.
-        """
-
-        if self.dataset_metadata is None:
-            raise ValueError("dataset_metadata is None.")
-
-        # only check the center frequency once it's been set. it is generated
-        # at baseband (f=0) first and then later updated once it reaches
-        # the dataset stage, when it can then be checked
-        if self._center_freq_set:
-            self.center_freq = verify_float(
-                self.center_freq,
-                name = "center_freq",
-                low = self.dataset_metadata.signal_center_freq_min,
-                high = self.dataset_metadata.signal_center_freq_max
-            )
-
-        self.bandwidth = verify_float(
-            self.bandwidth,
-            name = "bandwidth",
-            low = 0.0,
-            high = self.dataset_metadata.sample_rate,
-            exclude_low = True
-        )
-
-        self.start_in_samples = verify_int(
-            self.start_in_samples,
-            name = "start_in_samples",
-            low = 0,
-            high = self.dataset_metadata.num_iq_samples_dataset,
-            exclude_high = True
-        )
-
-        self.duration_in_samples = verify_int(
-            self.duration_in_samples,
-            name = "duration_in_samples",
-            low = 0,
-            high = self.dataset_metadata.num_iq_samples_dataset,
-            exclude_low = True
-        )
-
-        self.snr_db = verify_float(
-            self.snr_db,
-            name = "snr_db",
-            low = 0.0,
-        )
-
-        self.class_name = verify_str(
-            self.class_name,
-            name = "class_name",
-        )
-
-        self.class_index = verify_int(
-            self.class_index,
-            name = "class_index",
-            low = 0,
-        )
-
     def __repr__(self):
         class_dict = self.to_dict()
         params = [f"{k}={v}" for k,v in class_dict.items()]
         params_str = ",".join(params)
         return f"{self.__class__.__name__}({params_str})"
-        #return f"{self.__class__.__name__}(center_freq={self.center_freq}, bandwidth={self.bandwidth}, start_in_samples={self.start_in_samples}, duration_in_samples={self.duration_in_samples}, snr_db={self.snr_db}, class_name={self.class_name}, class_index={self.class_index})"
-
-class SignalMetadataExternal():
-    """Modified SignalMetadata with fewer structural requirements, suitable for importing incomplete external datasets.
-    """
-    def __init__(
-        self,
-        dataset_metadata: ExternalDatasetMetadata | DatasetMetadata = None,
-        **kwargs
-    ): 
-        """
-        Args:
-            dataset_metadata (ExternalDatasetMetadata, optional): Global metadata related to the dataset. Defaults to None.
-        """
-        self.dataset_metadata = dataset_metadata
-
-        # all metadata fields
-        for key,value in kwargs.items():
-            # do not overwrite these fields
-            if key not in ["sample_rate"]:
-                setattr(self, key, value)
-        
 
-        self.applied_transforms = []
-
-    @property
-    def sample_rate(self) -> float:
-        """Signal sample rate
-
-        Returns:
-            float: sample rate
-        """
-        return self.dataset_metadata.sample_rate
-
-    def to_dict(self) -> dict:
-        """Returns SignalMetadataExternal as a full dictionary
-        """
-        attributes_original = self.__dict__.copy()  # Start with the instance variables
-
-        for prop_name in dir(self):
-            prop = getattr(self.__class__, prop_name, None)
-            if isinstance(prop, property):
-                attributes_original[prop_name] = getattr(self, prop_name)
-
-        attributes = attributes_original.copy()
-
-        # exclude certain variables
-        for var in attributes_original:
-            if var in ["applied_transforms", "dataset_metadata", "_dataset_metadata"]:
-                del attributes[var]
-
-        return attributes
-
-    def deepcopy(self) -> SignalMetadataExternal:
-        """Returns a deep copy of itself
-
-        Returns:
-            SignalMetadataExternal: Deep copy of SignalMetadataExternal
-        """        
-        return copy.deepcopy(self)
-
-
-    def verify(self) -> None:
-        """Verifies Signal metadata fields
-
-        Raises:
-            MissingSignalMetadataExternal: Metadata missing.
-            InvalidSignalMetadataExternal: Metadata invalid.
-        """
-
-        if self.dataset_metadata is None:
-            raise ValueError("dataset_metadata is None.")
-
-    def __repr__(self):
-        # return f"{self.__class__.__name__}(class_name={self.class_name}, class_index={self.class_index})"
-        return generate_repr_str(self, exclude_params=["_dataset_metadata"])
+def targets_as_metadata(targets, target_labels, dataset_metadata=None):
+    """utility function for reading target labels as signal metadata objects; returns a new SignalMetadata"""
+    signal_metadata = SignalMetadata(dataset_metadata=dataset_metadata)
+    if not isinstance(targets,list):
+        targets = [targets]
+    for i in range(len(target_labels)):
+        setattr(signal_metadata, target_labels[i], targets[i])
+    return signal_metadata
 
+def dict_to_signal_metadata(metadata_dict, dataset_metadata=None):
+    return targets_as_metadata([metadata_dict[key] for key in metadata_dict.keys()], list(metadata_dict.keys()), dataset_metadata)
 
 ### Signal
 class Signal():
@@ -478,19 +333,25 @@ class Signal():
         """
     def __init__(
         self, 
-        data: np.ndarray = np.array([]), 
-        metadata: SignalMetadata | SignalMetadataExternal = None, 
-        component_signals: List[Signal] = []
+        data = np.array([]), 
+        metadata = None, 
+        component_signals: List[Signal] = [],
+        dataset_metadata = None,
     ):
         """Initializes the Signal with data and metadata.
 
         Args:
-            data (np.ndarray, optional): Signal IQ data. Defaults to np.array([]).
-            metadata (SignalMetadata | SignalMetadataExternal, optional): Signal metadata. Defaults to None.
-            component_signals (List[Signal], optional): individual components of the full signal, e.g. smaller individual signals collected together in a wideband signal. Defaults to [].
+            data: Signal IQ data. Defaults to np.array([])
+            metadata: Signal metadata; Defaults to None
+            component_signals (List[Signal], optional): individual components of the full signal, e.g. smaller individual signals collected together in a wideband signal. Defaults to []
+            dataset_metadata: overrides dataset_metadata for metadata; sometimes useful in constructors or custom file readers; generally safe to ignore
         """
         self.data = data
         self.metadata = metadata
+        if type(self.metadata) == dict:
+            self.metadata = dict_to_signal_metadata(self.metadata)
+        if dataset_metadata != None:
+            self.metadata.dataset_metadata = dataset_metadata
         self.component_signals = component_signals
 
     def verify(self):
@@ -514,7 +375,7 @@ class Signal():
     def get_full_metadata(self):
         """
         Returns a list of all top level metadata objects in the Signal. 
-        If no metadata is ddefined on a Signal, it's metadata is assumed to be the list of metadata of it's children.
+        If no metadata is defined on a Signal, it's metadata is assumed to be the list of metadata of it's children.
         This process is applied recursively until no more children without metadata can be found.
         """
         if not self.metadata is None:
diff --git a/torchsig/transforms/base_transforms.py b/torchsig/transforms/base_transforms.py
index 07bbe4504..c76b0f9a6 100644
--- a/torchsig/transforms/base_transforms.py
+++ b/torchsig/transforms/base_transforms.py
@@ -14,7 +14,7 @@ __all__ = [
 
 # TorchSig
 import torchsig.transforms.functional as F
-from torchsig.signals.signal_types import Signal, SignalMetadata, SignalMetadataExternal
+from torchsig.signals.signal_types import Signal, SignalMetadata
 from torchsig.utils.random import Seedable
 from torchsig.utils.printing import generate_repr_str
 
@@ -40,8 +40,8 @@ class Transform(ABC, Seedable):
 
     def __validate__(
         self, 
-        signal: Signal | SignalMetadata | SignalMetadataExternal
-    ) -> Signal | SignalMetadata | SignalMetadataExternal:
+        signal: Signal | SignalMetadata
+    ) -> Signal | SignalMetadata:
         """Validates signal or metadata before applying transform
 
         Args:
@@ -55,7 +55,7 @@ class Transform(ABC, Seedable):
         """        
         raise NotImplementedError
 
-    def __update__(self, signal: Signal | SignalMetadata | SignalMetadataExternal) -> None:
+    def __update__(self, signal: Signal | SignalMetadata) -> None:
         """Updates bookeeping for signals
 
         Args:
@@ -80,18 +80,18 @@ class Transform(ABC, Seedable):
 
             
 
-        elif isinstance(signal, (SignalMetadata, SignalMetadataExternal)):
-            # SignalMetadata or SignalMetadataExternal object
+        elif isinstance(signal, (SignalMetadata)):
+            # SignalMetadata object
             if signal is None:
                 raise ValueError(f"Invalid signal metadata object to update in transform {self.__class__.__name__}. Signal metadata is None: {signal}")
             signal.applied_transforms.append(self)
         else:
-            raise ValueError(f"Invalid signal metadata object to update in transform {self.__class__.__name__}. Must be Signal or SignalMetadata/SignalMetadataExternal, not {type(signal)}.")
+            raise ValueError(f"Invalid signal metadata object to update in transform {self.__class__.__name__}. Must be Signal or SignalMetadata, not {type(signal)}.")
 
     def __apply__(
         self, 
-        signal: Signal | SignalMetadata | SignalMetadataExternal
-    ) -> Signal | SignalMetadata | SignalMetadataExternal:  
+        signal: Signal | SignalMetadata
+    ) -> Signal | SignalMetadata:  
         """Performs transform
 
         Args:
@@ -108,7 +108,7 @@ class Transform(ABC, Seedable):
     def __call__(
         self, 
         signal: Signal | SignalMetadata
-    ) -> Signal | SignalMetadata | SignalMetadataExternal:
+    ) -> Signal | SignalMetadata:
         """Validate signal, performs transform, update bookeeping
 
         Args:
diff --git a/torchsig/transforms/metadata_transforms.py b/torchsig/transforms/metadata_transforms.py
index bcdf2e73b..851965e09 100755
--- a/torchsig/transforms/metadata_transforms.py
+++ b/torchsig/transforms/metadata_transforms.py
@@ -202,7 +202,7 @@ class YOLOLabel(MetadataTransform):
                 "start",
                 "bandwidth",
                 "center_freq",
-                "sample_rate"
+                "dataset_metadata"
             ],
             **kwargs
         )
@@ -214,11 +214,11 @@ class YOLOLabel(MetadataTransform):
         # normalized to width of sample
         width = signal.duration
         # normalize bandwidth with sample rate
-        height = signal.bandwidth/signal.sample_rate
+        height = signal.bandwidth/signal.dataset_metadata.sample_rate
         x_center = signal.start + (width / 2.0)
         # normalize center frequency with sample rate
         # subtract from 1 since (0,0) for YOLO is upper left, but we define (0,0) lower left
-        y_center = 1 - ((signal.sample_rate/2.0) + signal.center_freq) / signal.sample_rate
+        y_center = 1 - ((signal.dataset_metadata.sample_rate/2.0) + signal.center_freq) / signal.dataset_metadata.sample_rate
         yolo_label = (class_index, x_center, y_center, width, height)
         setattr(signal, "yolo_label", yolo_label)
 
diff --git a/torchsig/utils/file_handlers/__init__.py b/torchsig/utils/file_handlers/__init__.py
index ae9064404..54157b196 100644
--- a/torchsig/utils/file_handlers/__init__.py
+++ b/torchsig/utils/file_handlers/__init__.py
@@ -1,7 +1,6 @@
 """TorchSig File Handlers
 """
 from .base_handler import BaseFileHandler, FileReader, FileWriter
-from .external import ExternalFileHandler
 
 __all__ = [
     "BaseFileHandler",
diff --git a/torchsig/utils/file_handlers/external.py b/torchsig/utils/file_handlers/external.py
deleted file mode 100644
index 5cfca1839..000000000
--- a/torchsig/utils/file_handlers/external.py
+++ /dev/null
@@ -1,66 +0,0 @@
-"""External File Handler base class for user imported data
-"""
-
-from __future__ import annotations
-
-# Third Party
-import numpy as np
-
-# Built-In
-from typing import TYPE_CHECKING, Tuple, List
-
-if TYPE_CHECKING:
-    from torchsig.datasets.dataset_metadata import ExternalDatasetMetadata
-
-
-class ExternalFileHandler:
-    """Abstract base for user-provided file handlers in ExternalTorchSigDataset.
-
-    Users should subclass this and implement `size`, `load_dataset_metadata`,
-    and `load` to adapt external datasets into the TorchSig pipeline.
-    """   
-    def __init__(
-        self,
-        root: str,
-    ):
-        """Initialize with the external dataset root directory.
-
-        Args:
-            root (str): Path to the external dataset.
-        """
-        self.root = root
-
-    def size(self) -> int:
-        """Compute the number of samples in the external dataset.
-
-        Returns:
-            int: Total number of samples.
-
-        Raises:
-            NotImplementedError: Subclasses must implement this method.
-        """   
-        raise NotImplementedError
-
-    def load_dataset_metadata(self) -> ExternalDatasetMetadata:
-        """Load in dataset information into a `ExternalDatasetMetadata`.
-        Raises:
-            NotImplementedError: Subclasses must implement this method.
-
-        Returns:
-            ExternalDatasetMetadata: Dataset metadata.
-        """        
-        raise NotImplementedError
-
-    def load(self, idx: int) -> Tuple[np.ndarray, List[Any]]:
-        """Load a single sample from dataset on disk
-
-        Args:
-            idx (int): index of sample to load.
-
-        Raises:
-            NotImplementedError: Subclasses must implement this method.
-
-        Returns:
-            Tuple[np.ndarray, List[Any]]: data, targets
-        """        
-        raise NotImplementedError
\ No newline at end of file
diff --git a/torchsig/utils/writer.py b/torchsig/utils/writer.py
index 68cc79874..2f128c701 100644
--- a/torchsig/utils/writer.py
+++ b/torchsig/utils/writer.py
@@ -8,7 +8,7 @@ from torchsig.datasets.dataset_utils import dataset_yaml_name, writer_yaml_name
 from torchsig.utils.file_handlers.base_handler import FileWriter as TorchSigFileHandler
 from torchsig.utils.file_handlers.hdf5 import HDF5Writer as DEFAULT_FILE_HANDLER
 from torchsig.utils.yaml import write_dict_to_yaml
-from torchsig.signals.signal_types import Signal, SignalMetadata
+from torchsig.signals.signal_types import Signal, SignalMetadata, targets_as_metadata
 
 # Third Party
 from tqdm.auto import tqdm
@@ -43,15 +43,6 @@ def handle_non_numpy_datatypes(data):
             data = data.numpy()
         return data
 
-def targets_as_metadata(targets, target_labels, dataset_metadata):
-    """utility function for reading target labels as signal metadata objects; returns a new SignalMetadata"""
-    signal_metadata = SignalMetadata(dataset_metadata=dataset_metadata)
-    if not isinstance(targets,list):
-        targets = [targets]
-    for i in range(len(target_labels)):
-        setattr(signal_metadata, target_labels[i], targets[i])
-    return signal_metadata
-
 def batch_as_signal_list(batch, target_labels = None, dataset_metadata = None):
     signal_list = []
     if isinstance(batch, tuple) and len(batch) == 2:
