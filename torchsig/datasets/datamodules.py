"""PyTorch Lightning DataModules
Learn More: https://lightning.ai/docs/pytorch/stable/data/datamodule.html
If dataset does not exist at root, creates new dataset and writes to disk
If dataset does exist, simply loaded it back in
"""

from __future__ import annotations

# TorchSig
from torchsig.datasets.dataset_metadata import DatasetMetadata
from torchsig.datasets.datasets import TorchSigIterableDataset, StaticTorchSigDataset
from torchsig.datasets.dataset_utils import to_dataset_metadata
from torchsig.utils.writer import DatasetCreator
from torchsig.utils.file_handlers import BaseFileHandler
from torchsig.utils.file_handlers.hdf5 import HDF5Writer as DEFAULT_WRITER, HDF5Reader as DEFAULT_READER
from torchsig.transforms.base_transforms import Transform
from torchsig.transforms.metadata_transforms import MetadataTransform
from torchsig.transforms.impairments import Impairments
from torchsig.datasets.default_configs.loader import get_default_yaml_config

# Third Party
import pytorch_lightning as pl
from torch.utils.data import DataLoader, random_split
import numpy as np

# Built-In
from typing import Callable, List
from pathlib import Path

class TorchSigDataModule(pl.LightningDataModule):
    """PyTorch Lightning DataModule for creating and loading TorchSig datasets.

    This DataModule handles:
      - Dataset creation or loading from disk via a file handler.
      - Splitting into train/val/test subsets.
      - Batching, collation, and worker seeding for training.

    Attributes:
        root (Path): Directory where datasets are stored or created.
        dataset_size (int): Total number of samples in the dataset.
        dataset_splits (List[float] | List[int]): Fractions or counts for train/val/test splits.
        dataset_metadata (DatasetMetadata): Metadata describing the dataset.
        impairment_level (int | None): Optional interference level for synthetic impairments.
        transforms (List[Transform]): Transforms applied to the input data.
        component_transforms (List[Transform]): Transforms applied to individual signal components.
        target_labels (List[str] | None): Names of target metadata fields to include.
        batch_size (int): Batch size for the training/validation/testing DataLoaders.
        num_workers (int): Number of worker processes for data loading.
        collate_fn (Callable | None): Custom collate function for batching.
        create_batch_size (int): Batch size used during on-disk dataset creation.
        create_num_workers (int): Number of workers used during dataset creation.
        file_writer (Type[BaseFileHandler]): FileHandler class for disk I/O.
        file_reader (Type[BaseFileHandler]): FileReader class for disk I/O.
        overwrite (bool): If True, existing on-disk data will be overwritten.
        seed (int | None): Optional random seed for reproducibility.
        train (StaticTorchSigDataset): Initialized training dataset (set in `setup()`).
        val (StaticTorchSigDataset): Initialized validation dataset (set in `setup()`).
        test (StaticTorchSigDataset): Initialized test dataset (set in `setup()`).
    """
    def __init__(
        self,
        root: str,
        dataset_metadata: DatasetMetadata | str | dict,
        dataset_size: int,
        dataset_splits: List[float] | List[int] = [0.70, 0.20, 0.10],
        # dataloader params
        batch_size: int = 1,
        num_workers: int = 1,
        collate_fn: Callable = None,

        # dataset creator params
        create_batch_size: int = 8,
        create_num_workers: int = 4,
        file_writer: BaseFileHandler = DEFAULT_WRITER,
        file_reader: BaseFileHandler = DEFAULT_READER,
        overwrite: bool = False,
        # tranforms
        impairment_level: int = None,
        transforms: List[Transform] = [],
        component_transforms: List[Transform] = [],
        target_labels: List[str] = None,
        seed: int = None,
    ):
        """Initialize the TorchSigDataModule.

        Args:
            root (str): Path to store or load the dataset.
            dataset_metadata (DatasetMetadata | str | dict): Metadata object, YAML file path, or dict describing classes and settings.
            dataset_size (int): Total number of samples to generate or load.
            dataset_splits (List[float] | List[int], optional): Fractions or counts for train/val/test splits. Defaults to [0.70, 0.20, 0.10].
            batch_size (int, optional): Batch size for data loaders. Defaults to 1.
            num_workers (int, optional): Number of worker processes for data loading. Defaults to 1.
            collate_fn (Callable, optional): Custom function to collate batch samples. Defaults to None.
            create_batch_size (int, optional): Batch size when writing data to disk. Defaults to 8.
            create_num_workers (int, optional): Workers used when creating the on-disk dataset. Defaults to 4.
            file_writer (Type[BaseFileHandler]): FileWriter class for disk I/O.
            file_reader (Type[BaseFileHandler]): FileReader class for disk I/O.
            overwrite (bool, optional): If True, existing data at `root` will be overwritten. Defaults to False.
            impairment_level (int, optional): Level of synthetic impairment to apply. Defaults to None (no impairment).
            transforms (List[Transform], optional): List of transforms applied to each sampleâ€™s input. Defaults to [].
            component_transforms (List[Transform], optional): Transforms applied to individual signal components. Defaults to [].
            target_labels (List[str], optional): Names of metadata fields to include. Defaults to None.
            seed (int, optional): Seed for randomness and reproducibility. Defaults to None.
        """
        # read from yaml or dataset metadata or code inputs
        super().__init__()
        # filepaths
        self.root = Path(root)
        self.dataset_size = dataset_size
        self.dataset_splits = dataset_splits
        # metadatas
        self.dataset_metadata = to_dataset_metadata(dataset_metadata)
        # transforms
        self.impairment_level = impairment_level
        self.transforms = transforms
        self.component_transforms = component_transforms
        if self.impairment_level is not None:
            # add impairment transforms
            impairment_transforms = Impairments(level=self.impairment_level)
            self.transforms = impairment_transforms.dataset_transforms + self.transforms
            self.component_transforms = impairment_transforms.signal_transforms + self.component_transforms
        self.target_labels = target_labels
        # initialize dataloader params
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.collate_fn = collate_fn

        # dataset creator params
        self.create_batch_size = create_batch_size
        self.create_num_workers = create_num_workers
        self.file_writer = file_writer
        self.file_reader = file_reader
        self.overwrite = overwrite
        
        # to be initialized in setup()
        self.train: StaticTorchSigDataset = None
        self.val: StaticTorchSigDataset = None
        self.test: StaticTorchSigDataset = None
        self.seed = seed
        

    def prepare_data(self) -> None:
        """
        Prepares the dataset by creating new datasets if they do not exist on disk.
        The datasets are created using the `DatasetCreator` class.
        If the dataset already exists on disk, it is loaded back into memory.
        """
        dataset = TorchSigIterableDataset(
            dataset_metadata = self.dataset_metadata,
            transforms = self.transforms,
            seed=self.seed
        )
        loader = DataLoader(
            dataset = dataset,
            batch_size = self.create_batch_size,
            num_workers = self.create_num_workers,
            collate_fn = self.collate_fn
        )
        creator = DatasetCreator(
            dataloader = loader,
            dataset_length = self.dataset_size,
            root = self.root,
            overwrite = self.overwrite,
            file_writer = self.file_writer,
        )
        # breakpoint()
        print(f"Full Dataset: Impairment Level {self.impairment_level}, {self.dataset_size} dataset size")
        creator.create()

    def setup(self, stage: str = 'train') -> None:
        """
        Sets up the train and validation datasets for the given stage.
        Args:
            stage (str, optional): The stage of the DataModule, typically 'train' or 'test'. Defaults to 'train'.
        """
        full_dataset = StaticTorchSigDataset(
            root = self.root,
            file_handler_class=self.file_reader,
            target_labels=self.target_labels
        )
        self.train, self.val, self.test = random_split(full_dataset, self.dataset_splits)

    def train_dataloader(self) -> DataLoader:
        """
        Returns the DataLoader for the training dataset.
        Returns:
            DataLoader: A PyTorch DataLoader for the training dataset.
        """
        return DataLoader(
            dataset = self.train,
            batch_size = self.batch_size,
            shuffle = True,
            num_workers = self.num_workers,
            collate_fn = self.collate_fn
        )
    def val_dataloader(self) -> DataLoader:
        """
        Returns the DataLoader for the validation dataset.
        Returns:
            DataLoader: A PyTorch DataLoader for the validation dataset.
        """
        return DataLoader(
            dataset = self.val,
            batch_size = self.batch_size,
            shuffle = False,
            num_workers = self.num_workers,
            collate_fn = self.collate_fn
        )
    
    def test_dataloader(self) -> DataLoader:
        """
        Returns the DataLoader for the test dataset.
        Returns:
            DataLoader: A PyTorch DataLoader for the test dataset.
        """
        return DataLoader(
            dataset = self.test,
            batch_size = self.batch_size,
            shuffle = False,
            num_workers = self.num_workers,
            collate_fn = self.collate_fn
        )
### DataModules for Official Dataset
### uses default YAML configs in torchsig/datasets/default_configs
class OfficialTorchSigDataModule(TorchSigDataModule):
    """
    A PyTorch Lightning DataModule for official TorchSignal datasets.
    
    This class manages the dataset metadata, configuration, and data loading process 
    for datasets with official configurations instead of using custom metadata. 
    It initializes the train and validation metadata based on the dataset type and 
    impairment level.
    Args:
        root (str): Root directory where the dataset is stored.
        impairment_level (int): Defines the impairment level of the dataset.
        batch_size (int, optional): Batch size for the dataloaders. Default is 1.
        num_workers (int, optional): Number of workers for data loading. Default is 1.
        collate_fn (Callable, optional): Function to merge a list of samples into a batch. Default is None.
        create_batch_size (int, optional): Batch size used during dataset creation. Default is 8.
        create_num_workers (int, optional): Number of workers used during dataset creation. Default is 4.
        file_handler (BaseFileHandler, optional): File handler used to read/write dataset. Default is HDF5FileHandler.
        transforms (Transform | List[Callable | Transform], optional): List of transforms applied to dataset. Default is empty list.
        target_transforms (MetadataTransform | List[Callable | MetadataTransform], optional): List of transforms applied to targets. Default is empty list.
    """
    def __init__(
        self,
        root: str,
        impairment_level: int,

        # dataloader params
        batch_size: int = 1,
        num_workers: int = 1,
        collate_fn: Callable = None,

        # dataset creator params
        create_batch_size: int = 8,
        create_num_workers: int = 4,
        file_writer: BaseFileHandler = DEFAULT_WRITER,
        file_reader: BaseFileHandler = DEFAULT_READER,

        # applied after dataset written to disk
        transforms: Transform | List[Callable | Transform]  = [],
        target_transforms: MetadataTransform | List[Callable | MetadataTransform] = [],
        seed = None,
    ):
        # sets train and val metadata
        train_metadata = get_default_yaml_config(
            impairment_level = impairment_level,
            train = True
        )
        val_metadata = get_default_yaml_config(
            impairment_level = impairment_level,
            train = False
        )
        super().__init__(
            root = root,
            train_metadata = train_metadata,
            val_metadata = val_metadata,
            batch_size = batch_size,
            num_workers = num_workers,
            collate_fn = collate_fn,
            create_batch_size = create_batch_size,
            create_num_workers = create_num_workers,
            file_writer = file_writer,
            file_reader = file_reader,
            transforms = transforms,
            target_transforms = target_transforms,
            seed = seed,
        )